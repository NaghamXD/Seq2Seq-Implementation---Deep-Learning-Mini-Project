{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "70fc9e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "\n",
    "# pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import spacy\n",
    "from collections import Counter\n",
    "\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from torchmetrics.functional.text import bleu_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "93c9cbd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device mps\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "print(\"Device\", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6751a509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Set Seeds for Reproducibility\n",
    "SEED = 1234\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# Check if MPS is available and set the seed\n",
    "if torch.backends.mps.is_available():\n",
    "    torch.mps.manual_seed(SEED)\n",
    "\n",
    "# Check if CUDA is available (just in case you move this code to a server later)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "523fe585",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db59c67009ab4932a407d365ebf25904",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4df3911a54b4fa8bf8c157d9ab996e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['translation'],\n",
      "        num_rows: 40836715\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['translation'],\n",
      "        num_rows: 3000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['translation'],\n",
      "        num_rows: 3003\n",
      "    })\n",
      "})\n",
      "First training example: {'translation': {'en': 'Resumption of the session', 'fr': 'Reprise de la session'}}\n"
     ]
    }
   ],
   "source": [
    "# Download and load the WMT14 French-English dataset\n",
    "# This might take a few minutes as the dataset is large\n",
    "dataset = load_dataset(\"wmt14\", \"fr-en\")\n",
    "\n",
    "# Print the dataset structure to verify\n",
    "print(dataset)\n",
    "\n",
    "# Example: Inspect the first training example\n",
    "print(\"First training example:\", dataset['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "599d2df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_fr = spacy.load(\"fr_core_news_sm\")\n",
    "spacy_eng = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6811ce66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#First, we create a class to manage the mapping between words and IDs. This handles the \"80k vocabulary\" limit mentioned in the paper.\n",
    "\n",
    "class Vocabulary:\n",
    "    def __init__(self, freq_threshold=2, max_size=80000):\n",
    "        self.itos = {0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\", 3: \"<UNK>\"}\n",
    "        self.stoi = {\"<PAD>\": 0, \"<SOS>\": 1, \"<EOS>\": 2, \"<UNK>\": 3}\n",
    "        self.freq_threshold = freq_threshold\n",
    "        self.max_size = max_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.itos)\n",
    "\n",
    "    @staticmethod\n",
    "    def tokenizer_eng(text):\n",
    "        return [tok.text.lower() for tok in spacy_eng.tokenizer(text)]\n",
    "\n",
    "    @staticmethod\n",
    "    def tokenizer_fr(text):\n",
    "        return [tok.text.lower() for tok in spacy_fr.tokenizer(text)]\n",
    "\n",
    "    def build_vocabulary(self, sentence_list, tokenizer):\n",
    "        frequencies = Counter()\n",
    "        idx = 4 # Start index after special tokens\n",
    "\n",
    "        # 1. Count frequencies of all words\n",
    "        for sentence in sentence_list:\n",
    "            for word in tokenizer(sentence):\n",
    "                frequencies[word] += 1\n",
    "\n",
    "        # 2. Sort by frequency and keep top 'max_size' words\n",
    "        # This matches the paper's strategy of capping vocab size\n",
    "        common_words = frequencies.most_common(self.max_size - 4)\n",
    "\n",
    "        # 3. Add valid words to our dictionary\n",
    "        for word, count in common_words:\n",
    "            if count >= self.freq_threshold:\n",
    "                self.stoi[word] = idx\n",
    "                self.itos[idx] = word\n",
    "                idx += 1\n",
    "\n",
    "    def numericalize(self, text, tokenizer):\n",
    "        tokenized_text = tokenizer(text)\n",
    "        \n",
    "        return [\n",
    "            self.stoi[token] if token in self.stoi else self.stoi[\"<UNK>\"]\n",
    "            for token in tokenized_text\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7031c14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we define a PyTorch Dataset that takes the raw Hugging Face data and converts it into numbers using the Vocabulary class above.\n",
    "\n",
    "class WMT14Dataset_regular_order(Dataset):\n",
    "    def __init__(self, hf_dataset, source_vocab, target_vocab):\n",
    "        self.hf_dataset = hf_dataset\n",
    "        self.source_vocab = source_vocab\n",
    "        self.target_vocab = target_vocab\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.hf_dataset)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Get the pair: {'fr': '...', 'en': '...'}\n",
    "        pair = self.hf_dataset[index]['translation']\n",
    "        src_text = pair['en']\n",
    "        trg_text = pair['fr']\n",
    "\n",
    "        # Convert text to indices\n",
    "        # Add <SOS> at start and <EOS> at end\n",
    "        numericalized_source = [self.source_vocab.stoi[\"<SOS>\"]]\n",
    "        numericalized_source += self.source_vocab.numericalize(src_text, self.source_vocab.tokenizer_eng)\n",
    "        numericalized_source.append(self.source_vocab.stoi[\"<EOS>\"])\n",
    "\n",
    "        numericalized_target = [self.target_vocab.stoi[\"<SOS>\"]]\n",
    "        numericalized_target += self.target_vocab.numericalize(trg_text, self.target_vocab.tokenizer_fr)\n",
    "        numericalized_target.append(self.target_vocab.stoi[\"<EOS>\"])\n",
    "\n",
    "        return torch.tensor(numericalized_source), torch.tensor(numericalized_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "94baabeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WMT14Dataset(Dataset):\n",
    "    def __init__(self, hf_dataset, source_vocab, target_vocab):\n",
    "        self.hf_dataset = hf_dataset\n",
    "        self.source_vocab = source_vocab\n",
    "        self.target_vocab = target_vocab\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.hf_dataset)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        pair = self.hf_dataset[index]['translation']\n",
    "        src_text = pair['en']\n",
    "        trg_text = pair['fr']\n",
    "\n",
    "        # 1. Numericalize Source\n",
    "        # Get list of numbers: [4, 10, 55] (\"the\", \"cat\", \"sat\")\n",
    "        src_indices = self.source_vocab.numericalize(src_text, self.source_vocab.tokenizer_eng)\n",
    "        \n",
    "        # --- IMPLEMENTATION OF PAPER POINT #1: REVERSE INPUT ---\n",
    "        # Reverse the list: [55, 10, 4] (\"sat\", \"cat\", \"the\")\n",
    "        src_indices = src_indices[::-1] \n",
    "        \n",
    "        # Add special tokens\n",
    "        numericalized_source = [self.source_vocab.stoi[\"<SOS>\"]]\n",
    "        numericalized_source += src_indices\n",
    "        numericalized_source.append(self.source_vocab.stoi[\"<EOS>\"])\n",
    "\n",
    "        # 2. Numericalize Target (Do NOT reverse this)\n",
    "        trg_indices = self.target_vocab.numericalize(trg_text, self.target_vocab.tokenizer_fr)\n",
    "        \n",
    "        numericalized_target = [self.target_vocab.stoi[\"<SOS>\"]]\n",
    "        numericalized_target += trg_indices\n",
    "        numericalized_target.append(self.target_vocab.stoi[\"<EOS>\"])\n",
    "\n",
    "        return torch.tensor(numericalized_source), torch.tensor(numericalized_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9d48cc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Since sentences have different lengths, we cannot simply stack them into a matrix.\n",
    "# We need a specific function (called collate_fn) to pad short sentences with zeros (the <PAD> token) so that every batch is rectangular.\n",
    "\n",
    "class MyCollate:\n",
    "    def __init__(self, pad_idx):\n",
    "        self.pad_idx = pad_idx\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        source = [item[0] for item in batch]\n",
    "        target = [item[1] for item in batch]\n",
    "\n",
    "        # Pad sequences to the max length in this batch\n",
    "        source = pad_sequence(source, batch_first=False, padding_value=self.pad_idx)\n",
    "        target = pad_sequence(target, batch_first=False, padding_value=self.pad_idx)\n",
    "\n",
    "        return source, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "74f7f75c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Subset Size: 10000\n",
      "Valid Subset Size: 1000\n",
      "Building English Vocabulary...\n",
      "Building French Vocabulary...\n",
      "Testing the pipeline...\n",
      "Source Shape: torch.Size([80, 32])\n",
      "Target Shape: torch.Size([86, 32])\n"
     ]
    }
   ],
   "source": [
    "# 1. Load Data\n",
    "# Select subsets: 10k for training, 1k for validation\n",
    "train_subset = dataset['train'].select(range(10000))\n",
    "valid_subset = dataset['validation'].select(range(1000))\n",
    "\n",
    "print(f\"Train Subset Size: {len(train_subset)}\")\n",
    "print(f\"Valid Subset Size: {len(valid_subset)}\")\n",
    "\n",
    "# 2. Build Vocabulary\n",
    "print(\"Building English Vocabulary...\")\n",
    "english_sentences = [item['translation']['en'] for item in train_subset]\n",
    "vocab_en = Vocabulary(freq_threshold=1, max_size=80000) # Lowered freq_threshold for smaller dataset 10k\n",
    "vocab_en.build_vocabulary(english_sentences, vocab_en.tokenizer_eng)\n",
    "\n",
    "\n",
    "print(\"Building French Vocabulary...\")\n",
    "french_sentences = [item['translation']['fr'] for item in train_subset]\n",
    "vocab_fr = Vocabulary(freq_threshold=1, max_size=80000) # Lowered freq_threshold for smaller dataset 10k\n",
    "vocab_fr.build_vocabulary(french_sentences, vocab_fr.tokenizer_fr)\n",
    "\n",
    "# 3. Create Dataset\n",
    "train_subsetset = WMT14Dataset(train_subset, vocab_en, vocab_fr)\n",
    "valid_subsetset = WMT14Dataset(valid_subset, vocab_en, vocab_fr)\n",
    "\n",
    "# 4. Create DataLoaders\n",
    "BATCH_SIZE = 32 # Reduced batch size for smaller dataset 10k\n",
    "pad_idx = vocab_en.stoi[\"<PAD>\"]\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_subsetset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=0,\n",
    "    shuffle=True,\n",
    "    collate_fn=MyCollate(pad_idx=pad_idx)\n",
    ")\n",
    "\n",
    "# 5. Test it\n",
    "print(\"Testing the pipeline...\")\n",
    "for src_batch, trg_batch in train_loader:\n",
    "    print(f\"Source Shape: {src_batch.shape}\") # Expect [Seq_Len, Batch_Size]\n",
    "    print(f\"Target Shape: {trg_batch.shape}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "def664d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, num_layers, p):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = nn.Dropout(p)\n",
    "        self.embedding = nn.Embedding(input_size, embedding_size) # dropiut after embedding corrupts the input word vectors to prevent reliance on specific features.\n",
    "        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=p) # LSTM layer, dropout between layers prevents deeper\n",
    "                                                                               #layers from co-adapting too strongly with shallower layers.\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape : (seq_length, Batch_size) -> seq_length is the length of the input sentence, and we process the entire sequence at once\n",
    "        embedding = self.dropout(self.embedding(x))\n",
    "        # embedding shape : (seq_length, Batch_size, embedding_size)\n",
    "        outputs, (hidden, cell) = self.rnn(embedding)\n",
    "        # outputs shape : (seq_length, Batch_size, hidden_size)\n",
    "        # hidden shape : (num_layers, Batch_size, hidden_size)\n",
    "        # cell shape : (num_layers, Batch_size, hidden_size)\n",
    "        return hidden, cell\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, output_size, num_layers, p):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = nn.Dropout(p)\n",
    "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=p)\n",
    "        self.fc_out = nn.Linear(hidden_size, output_size) #output_size = input_size of the decoder = size of target vocabulary\n",
    "        self.output_size = output_size\n",
    "        \n",
    "    def forward(self, x, hidden, cell):\n",
    "        # x shape : (1, Batch_size)  -> we process one time step at a time\n",
    "        x = x.unsqueeze(0) #that's why we added one dimention \n",
    "        # x shape : (1, Batch_size, 1)\n",
    "        embedding = self.dropout(self.embedding(x))\n",
    "        # embedding shape : (1, Batch_size, embedding_size)\n",
    "        outputs, (hidden, cell) = self.rnn(embedding, (hidden, cell))\n",
    "        # outputs shape : (1, Batch_size, hidden_size)\n",
    "        predictions = self.fc_out(outputs.squeeze(0)) #remove the time step dimension for the linear layer\n",
    "        # predictions shape : (Batch_size, output_size)\n",
    "        return predictions, hidden, cell\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        \n",
    "    def forward(self, source, target, teacher_force_ratio=0.5):\n",
    "        batch_size = source.shape[1]\n",
    "        target_len = target.shape[0]\n",
    "        target_vocab_size = self.decoder.output_size\n",
    "\n",
    "        outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(device)\n",
    "\n",
    "        hidden, cell = self.encoder(source)\n",
    "\n",
    "        # First input to the decoder is the <SOS> tokens\n",
    "        x = target[0,:]  # shape: (Batch_size)\n",
    "\n",
    "        for t in range(1, target_len):\n",
    "            output, hidden, cell = self.decoder(x, hidden, cell)\n",
    "            outputs[t] = output\n",
    "\n",
    "            # Decide if we are going to use teacher forcing or not\n",
    "            teacher_force = random.random() < teacher_force_ratio\n",
    "\n",
    "            # Get the highest predicted token from our predictions\n",
    "            best_guess = output.argmax(1) \n",
    "\n",
    "            # If teacher forcing, use actual next token as next input; if not, use predicted token\n",
    "            x = target[t] if teacher_force else best_guess\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "50612aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to initialize weights\n",
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.uniform_(param.data, -0.08, 0.08)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a117841e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Architecture Specs from Paper ---\n",
    "INPUT_DIM = len(vocab_en)\n",
    "OUTPUT_DIM = len(vocab_fr)\n",
    "ENC_EMB_DIM = 1000  # Paper used 1000\n",
    "DEC_EMB_DIM = 1000  # Paper used 1000\n",
    "HID_DIM = 1000      # Paper used 1000\n",
    "N_LAYERS = 4        # Paper used 4\n",
    "DROPOUT = 0.2       # Paper implies some regularization, usually 0.2 is safe\n",
    "\n",
    "# --- Setup ---\n",
    "# Use the correct index for <PAD> from your English vocabulary\n",
    "TRG_PAD_IDX = vocab_en.stoi[\"<PAD>\"]\n",
    "\n",
    "# Create Model\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, DROPOUT)\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, OUTPUT_DIM, N_LAYERS, DROPOUT)\n",
    "model = Seq2Seq(enc, dec, DEVICE).to(DEVICE)\n",
    "\n",
    "# Initialize Weights\n",
    "model.apply(init_weights)\n",
    "\n",
    "# --- Optimization Specs ---\n",
    "BATCH_SIZE = 32    # Paper used 128\n",
    "LEARNING_RATE = 0.0005 # Paper used fixed 0.7 initially\n",
    "CLIP = 5            # Paper threshold for gradient norm\n",
    "TOTAL_EPOCHS = 20 # Paper trained for 7.5 epochs\n",
    "# Optimizer: SGD without momentum\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=TRG_PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2d62bcbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Loss: 6.084\n",
      "Epoch: 02 | Loss: 5.792\n",
      "Epoch: 03 | Loss: 5.610\n",
      "Epoch: 04 | Loss: 5.482\n",
      "Epoch: 05 | Loss: 5.377\n",
      "Epoch: 06 | Loss: 5.288\n",
      "Epoch: 07 | Loss: 5.233\n",
      "Epoch: 08 | Loss: 5.163\n",
      "Epoch: 09 | Loss: 5.106\n",
      "Epoch: 10 | Loss: 5.042\n",
      "Epoch: 11 | Loss: 5.000\n",
      "Epoch: 12 | Loss: 4.938\n",
      "Epoch: 13 | Loss: 4.895\n",
      "Epoch: 14 | Loss: 4.856\n",
      "Epoch: 15 | Loss: 4.820\n",
      "Epoch: 16 | Loss: 4.782\n",
      "Epoch: 17 | Loss: 4.736\n",
      "Epoch: 18 | Loss: 4.678\n",
      "Epoch: 19 | Loss: 4.642\n",
      "Epoch: 20 | Loss: 4.622\n",
      "--> Saved One-LSTM.pt\n"
     ]
    }
   ],
   "source": [
    "# --- Training Loop ---\n",
    "for epoch in range(TOTAL_EPOCHS):\n",
    "    \n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    # We need to know when we are \"halfway\" through\n",
    "    num_batches = len(train_loader)\n",
    "    halfway_point = num_batches // 2\n",
    "    \n",
    "    for i, (src, trg) in enumerate(train_loader):\n",
    "        \n",
    "        # Standard Training Step\n",
    "        src, trg = src.to(DEVICE), trg.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, trg)\n",
    "        \n",
    "        output_dim = output.shape[-1]\n",
    "        output = output[1:].view(-1, output_dim)\n",
    "        trg = trg[1:].view(-1)\n",
    "        \n",
    "        loss = criterion(output, trg)\n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), CLIP)\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    print(f'Epoch: {epoch+1:02} | Loss: {epoch_loss / len(train_loader):.3f}')\n",
    "\n",
    "# Save Model\n",
    "save_path = f\"One-LSTM.pt\"\n",
    "torch.save(model.state_dict(), save_path)\n",
    "print(f\"--> Saved {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dece8434",
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_search_decode_single(model, sentence, vocab_src, vocab_trg, beam_size=2, max_len=50, device='cpu'):\n",
    "    model.eval()\n",
    "    \n",
    "    # 1. Prepare Source\n",
    "    if isinstance(sentence, str):\n",
    "        tokens = vocab_src.tokenizer_eng(sentence)\n",
    "    else:\n",
    "        tokens = [token.lower() for token in sentence]\n",
    "        \n",
    "    indices = [vocab_src.stoi.get(t, vocab_src.stoi[\"<UNK>\"]) for t in tokens]\n",
    "    indices = indices[::-1] # Reverse Input\n",
    "    indices = [vocab_src.stoi[\"<SOS>\"]] + indices + [vocab_src.stoi[\"<EOS>\"]]\n",
    "    \n",
    "    src_tensor = torch.LongTensor(indices).unsqueeze(1).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        encoder_hidden, encoder_cell = model.encoder(src_tensor)\n",
    "\n",
    "        # Hypothesis: (Score, [Sequence], Hidden, Cell)\n",
    "        hypotheses = [(0.0, [vocab_trg.stoi[\"<SOS>\"]], encoder_hidden, encoder_cell)]\n",
    "\n",
    "        for _ in range(max_len):\n",
    "            all_candidates = []\n",
    "            \n",
    "            for score, seq, hidden, cell in hypotheses:\n",
    "                if seq[-1] == vocab_trg.stoi[\"<EOS>\"]:\n",
    "                    all_candidates.append((score, seq, hidden, cell))\n",
    "                    continue\n",
    "                \n",
    "                input_tensor = torch.LongTensor([seq[-1]]).to(device)\n",
    "                \n",
    "                # Predict\n",
    "                prediction, new_h, new_c = model.decoder(input_tensor, hidden, cell)\n",
    "                \n",
    "                # prediction is [1, vocab_size]. We squeeze to make it [vocab_size]\n",
    "                prediction = prediction.squeeze(0) \n",
    "                \n",
    "                # Log Softmax over dimension 0 (the vocabulary)\n",
    "                log_probs = F.log_softmax(prediction, dim=0)\n",
    "                \n",
    "                # Get Top K\n",
    "                top_k_probs, top_k_ids = log_probs.topk(beam_size * 2)\n",
    "                \n",
    "\n",
    "                # Expand\n",
    "                for i in range(len(top_k_ids)):\n",
    "                    word_idx = top_k_ids[i].item()\n",
    "                    prob = top_k_probs[i].item()\n",
    "                    all_candidates.append((score + prob, seq + [word_idx], new_h, new_c))\n",
    "\n",
    "            # Prune\n",
    "            hypotheses = sorted(all_candidates, key=lambda x: x[0], reverse=True)[:beam_size]\n",
    "            \n",
    "            if all(h[1][-1] == vocab_trg.stoi[\"<EOS>\"] for h in hypotheses):\n",
    "                break\n",
    "\n",
    "    best_seq = hypotheses[0][1]\n",
    "    decoded_words = [vocab_trg.itos[idx] for idx in best_seq]\n",
    "    \n",
    "    if \"<SOS>\" in decoded_words: decoded_words.remove(\"<SOS>\")\n",
    "    if \"<EOS>\" in decoded_words: decoded_words = decoded_words[:decoded_words.index(\"<EOS>\")]\n",
    "        \n",
    "    return \" \".join(decoded_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b0197511",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_bleu(data_subset, model, vocab_src, vocab_trg, device, beam_size=2):\n",
    "    targets = []      # Ground Truths\n",
    "    predictions = []  # Model Outputs\n",
    "    \n",
    "    print(f\"Starting BLEU Evaluation on {len(data_subset)} samples (Beam={beam_size})...\")\n",
    "    \n",
    "    for i, datum in enumerate(data_subset):\n",
    "        if (i + 1) % 50 == 0:\n",
    "            print(f\"Processed {i + 1} sentences...\")\n",
    "\n",
    "        pair = datum['translation']\n",
    "        src_text = pair['en']\n",
    "        trg_text = pair['fr']\n",
    "        \n",
    "        # 1. Get Prediction\n",
    "        pred_sentence = beam_search_decode_single(\n",
    "            model,          \n",
    "            src_text,       \n",
    "            vocab_src, \n",
    "            vocab_trg, \n",
    "            beam_size, \n",
    "            max_len=50,\n",
    "            device=device\n",
    "        )\n",
    "        \n",
    "        # 2. Collect Data\n",
    "        # Targets must be a list of lists: [['ref_sentence']]\n",
    "        targets.append([trg_text])\n",
    "        predictions.append(pred_sentence)\n",
    "\n",
    "    # 3. Compute Score\n",
    "    print(\"Computing Score...\")\n",
    "    score = bleu_score(predictions, targets, n_gram=1)\n",
    "    return score.item() * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a8619a07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting BLEU Evaluation on 1000 samples (Beam=2)...\n",
      "Processed 50 sentences...\n",
      "Processed 100 sentences...\n",
      "Processed 150 sentences...\n",
      "Processed 200 sentences...\n",
      "Processed 250 sentences...\n",
      "Processed 300 sentences...\n",
      "Processed 350 sentences...\n",
      "Processed 400 sentences...\n",
      "Processed 450 sentences...\n",
      "Processed 500 sentences...\n",
      "Processed 550 sentences...\n",
      "Processed 600 sentences...\n",
      "Processed 650 sentences...\n",
      "Processed 700 sentences...\n",
      "Processed 750 sentences...\n",
      "Processed 800 sentences...\n",
      "Processed 850 sentences...\n",
      "Processed 900 sentences...\n",
      "Processed 950 sentences...\n",
      "Processed 1000 sentences...\n",
      "Computing Score...\n",
      "Final Single-Model BLEU: 8.16\n"
     ]
    }
   ],
   "source": [
    "# Run this after training\n",
    "final_score = evaluate_bleu(\n",
    "    valid_subset,\n",
    "    model,             \n",
    "    vocab_en, \n",
    "    vocab_fr, \n",
    "    DEVICE,            \n",
    "    beam_size=2\n",
    ")\n",
    "\n",
    "print(f\"Final Single-Model BLEU: {final_score:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a6470da4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- DIAGNOSIS: TRAINING SET CHECK ---\n",
      "Starting BLEU Evaluation on 100 samples (Beam=2)...\n",
      "Processed 50 sentences...\n",
      "Processed 100 sentences...\n",
      "Computing Score...\n",
      "Training Set BLEU: 0.00\n"
     ]
    }
   ],
   "source": [
    "# Create a tiny subset of the TRAINING data (which the model has seen)\n",
    "train_debug_subset = dataset['train'].select(range(100))\n",
    "\n",
    "print(\"--- DIAGNOSIS: TRAINING SET CHECK ---\")\n",
    "train_score = evaluate_bleu(\n",
    "    train_debug_subset, \n",
    "    model, \n",
    "    vocab_en, \n",
    "    vocab_fr, \n",
    "    DEVICE, \n",
    "    beam_size=2\n",
    ")\n",
    "print(f\"Training Set BLEU: {train_score:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "54888c56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- DIAGNOSIS: VISUAL INSPECTION ---\n",
      "Input:  The buyer pays at an ATM.\n",
      "Target: L'acheteur effectue le paiement sur les bornes automatiques.\n",
      "Output: il ' est la question de\n",
      "------------------------------\n",
      "Input:  The most tragic section is the children's memorial, built in memory of 1.5 million children killed in concentration camps and gas chambers.\n",
      "Target: La partie la plus tragique est le mémorial des enfants, construit à la mémoire des 1,5 million d'enfants exterminés dans les camps de concentration et les chambres à gaz.\n",
      "Output: il ' est la la de la la de de la de de de , de , de , , , de de de et de de .\n",
      "------------------------------\n",
      "Input:  To force it to think that it feels something that it should be feeling when it sees something?\n",
      "Target: Lui faire croire qu'il ressent ce qu'il devrait normalement ressentir au moment où il voit quelque chose?\n",
      "Output: il ' est , une de de la , de la , de la de de la de .\n",
      "------------------------------\n",
      "Input:  Nor will trials of civilians will be banned in military tribunals, as requested by associations for the defence of human rights.\n",
      "Target: Pas plus que ne seront interdits les procès de civils dans des tribunaux militaires, comme le demandaient les associations de défense des droits de l'homme.\n",
      "Output: il ' est , à la de de la de de la , de la , de la et de la de de\n",
      "------------------------------\n",
      "Input:  The key is to make the right decision once cancer has been detected.\n",
      "Target: La clé est de prendre la bonne décision une fois qu'on a détecté un cancer.\n",
      "Output: il ' est , une de de la de de la de .\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "print(f\"--- DIAGNOSIS: VISUAL INSPECTION ---\")\n",
    "\n",
    "for i in range(5):\n",
    "    idx = random.randint(0, len(valid_subset)-1)\n",
    "    pair = valid_subset[idx]['translation']\n",
    "    src = pair['en']\n",
    "    trg = pair['fr']\n",
    "    \n",
    "    pred = beam_search_decode_single(model, src, vocab_en, vocab_fr, beam_size=12, device=DEVICE)\n",
    "    \n",
    "    print(f\"Input:  {src}\")\n",
    "    print(f\"Target: {trg}\")\n",
    "    print(f\"Output: {pred}\")\n",
    "    print(\"-\" * 30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DLC",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
