{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "70fc9e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "# pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as f\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import spacy\n",
    "from collections import Counter\n",
    "\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "93c9cbd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device mps\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "print(\"Device\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6751a509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Set Seeds for Reproducibility\n",
    "SEED = 1234\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# Check if MPS is available and set the seed\n",
    "if torch.backends.mps.is_available():\n",
    "    torch.mps.manual_seed(SEED)\n",
    "\n",
    "# Check if CUDA is available (just in case you move this code to a server later)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "523fe585",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d267be36580b4b27af350f0690d99578",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b7e447e7742407cb74fbf2d076f621e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['translation'],\n",
      "        num_rows: 40836715\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['translation'],\n",
      "        num_rows: 3000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['translation'],\n",
      "        num_rows: 3003\n",
      "    })\n",
      "})\n",
      "First training example: {'translation': {'en': 'Resumption of the session', 'fr': 'Reprise de la session'}}\n"
     ]
    }
   ],
   "source": [
    "# Download and load the WMT14 French-English dataset\n",
    "# This might take a few minutes as the dataset is large\n",
    "dataset = load_dataset(\"wmt14\", \"fr-en\")\n",
    "\n",
    "# Print the dataset structure to verify\n",
    "print(dataset)\n",
    "\n",
    "# Example: Inspect the first training example\n",
    "print(\"First training example:\", dataset['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "599d2df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_fr = spacy.load(\"fr_core_news_sm\")\n",
    "spacy_eng = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6811ce66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#First, we create a class to manage the mapping between words and IDs. This handles the \"80k vocabulary\" limit mentioned in the paper.\n",
    "\n",
    "class Vocabulary:\n",
    "    def __init__(self, freq_threshold=2, max_size=80000):\n",
    "        self.itos = {0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\", 3: \"<UNK>\"}\n",
    "        self.stoi = {\"<PAD>\": 0, \"<SOS>\": 1, \"<EOS>\": 2, \"<UNK>\": 3}\n",
    "        self.freq_threshold = freq_threshold\n",
    "        self.max_size = max_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.itos)\n",
    "\n",
    "    @staticmethod\n",
    "    def tokenizer_eng(text):\n",
    "        return [tok.text.lower() for tok in spacy_eng.tokenizer(text)]\n",
    "\n",
    "    @staticmethod\n",
    "    def tokenizer_fr(text):\n",
    "        return [tok.text.lower() for tok in spacy_fr.tokenizer(text)]\n",
    "\n",
    "    def build_vocabulary(self, sentence_list, tokenizer):\n",
    "        frequencies = Counter()\n",
    "        idx = 4 # Start index after special tokens\n",
    "\n",
    "        # 1. Count frequencies of all words\n",
    "        for sentence in sentence_list:\n",
    "            for word in tokenizer(sentence):\n",
    "                frequencies[word] += 1\n",
    "\n",
    "        # 2. Sort by frequency and keep top 'max_size' words\n",
    "        # This matches the paper's strategy of capping vocab size\n",
    "        common_words = frequencies.most_common(self.max_size - 4)\n",
    "\n",
    "        # 3. Add valid words to our dictionary\n",
    "        for word, count in common_words:\n",
    "            if count >= self.freq_threshold:\n",
    "                self.stoi[word] = idx\n",
    "                self.itos[idx] = word\n",
    "                idx += 1\n",
    "\n",
    "    def numericalize(self, text, tokenizer):\n",
    "        tokenized_text = tokenizer(text)\n",
    "        \n",
    "        return [\n",
    "            self.stoi[token] if token in self.stoi else self.stoi[\"<UNK>\"]\n",
    "            for token in tokenized_text\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7031c14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we define a PyTorch Dataset that takes the raw Hugging Face data and converts it into numbers using the Vocabulary class above.\n",
    "\n",
    "class WMT14Dataset(Dataset):\n",
    "    def __init__(self, hf_dataset, source_vocab, target_vocab):\n",
    "        self.hf_dataset = hf_dataset\n",
    "        self.source_vocab = source_vocab\n",
    "        self.target_vocab = target_vocab\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.hf_dataset)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Get the pair: {'fr': '...', 'en': '...'}\n",
    "        pair = self.hf_dataset[index]['translation']\n",
    "        src_text = pair['en']\n",
    "        trg_text = pair['fr']\n",
    "\n",
    "        # Convert text to indices\n",
    "        # Add <SOS> at start and <EOS> at end\n",
    "        numericalized_source = [self.source_vocab.stoi[\"<SOS>\"]]\n",
    "        numericalized_source += self.source_vocab.numericalize(src_text, self.source_vocab.tokenizer_eng)\n",
    "        numericalized_source.append(self.source_vocab.stoi[\"<EOS>\"])\n",
    "\n",
    "        numericalized_target = [self.target_vocab.stoi[\"<SOS>\"]]\n",
    "        numericalized_target += self.target_vocab.numericalize(trg_text, self.target_vocab.tokenizer_fr)\n",
    "        numericalized_target.append(self.target_vocab.stoi[\"<EOS>\"])\n",
    "\n",
    "        return torch.tensor(numericalized_source), torch.tensor(numericalized_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9d48cc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Since sentences have different lengths, we cannot simply stack them into a matrix.\n",
    "# We need a specific function (called collate_fn) to pad short sentences with zeros (the <PAD> token) so that every batch is rectangular.\n",
    "\n",
    "class MyCollate:\n",
    "    def __init__(self, pad_idx):\n",
    "        self.pad_idx = pad_idx\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        source = [item[0] for item in batch]\n",
    "        target = [item[1] for item in batch]\n",
    "\n",
    "        # Pad sequences to the max length in this batch\n",
    "        source = pad_sequence(source, batch_first=False, padding_value=self.pad_idx)\n",
    "        target = pad_sequence(target, batch_first=False, padding_value=self.pad_idx)\n",
    "\n",
    "        return source, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "74f7f75c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Subset Size: 10000\n",
      "Valid Subset Size: 1000\n",
      "Building English Vocabulary...\n",
      "Building French Vocabulary...\n",
      "Testing the pipeline...\n",
      "Source Shape: torch.Size([80, 32])\n",
      "Target Shape: torch.Size([86, 32])\n"
     ]
    }
   ],
   "source": [
    "# 1. Load Data\n",
    "# Select subsets: 10k for training, 1k for validation\n",
    "train_subset = dataset['train'].select(range(10000))\n",
    "valid_subset = dataset['validation'].select(range(1000))\n",
    "\n",
    "print(f\"Train Subset Size: {len(train_subset)}\")\n",
    "print(f\"Valid Subset Size: {len(valid_subset)}\")\n",
    "\n",
    "# 2. Build Vocabulary\n",
    "print(\"Building English Vocabulary...\")\n",
    "english_sentences = [item['translation']['en'] for item in train_subset]\n",
    "vocab_en = Vocabulary(freq_threshold=1, max_size=80000) # Lowered freq_threshold for smaller dataset 10k\n",
    "vocab_en.build_vocabulary(english_sentences, vocab_en.tokenizer_eng)\n",
    "\n",
    "\n",
    "print(\"Building French Vocabulary...\")\n",
    "french_sentences = [item['translation']['fr'] for item in train_subset]\n",
    "vocab_fr = Vocabulary(freq_threshold=1, max_size=80000) # Lowered freq_threshold for smaller dataset 10k\n",
    "vocab_fr.build_vocabulary(french_sentences, vocab_fr.tokenizer_fr)\n",
    "\n",
    "# 3. Create Dataset\n",
    "train_subsetset = WMT14Dataset(train_subset, vocab_en, vocab_fr)\n",
    "valid_subsetset = WMT14Dataset(valid_subset, vocab_en, vocab_fr)\n",
    "\n",
    "# 4. Create DataLoaders\n",
    "BATCH_SIZE = 32 # Reduced batch size for smaller dataset 10k\n",
    "pad_idx = vocab_en.stoi[\"<PAD>\"]\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_subsetset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=0,\n",
    "    shuffle=True,\n",
    "    collate_fn=MyCollate(pad_idx=pad_idx)\n",
    ")\n",
    "\n",
    "# 5. Test it\n",
    "print(\"Testing the pipeline...\")\n",
    "for src_batch, trg_batch in train_loader:\n",
    "    print(f\"Source Shape: {src_batch.shape}\") # Expect [Seq_Len, Batch_Size]\n",
    "    print(f\"Target Shape: {trg_batch.shape}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def664d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, num_layers, p):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = nn.Dropout(p)\n",
    "        self.embedding = nn.Embedding(input_size, embedding_size) # dropiut after embedding corrupts the input word vectors to prevent reliance on specific features.\n",
    "        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=p) # LSTM layer, dropout between layers prevents deeper\n",
    "                                                                               #layers from co-adapting too strongly with shallower layers.\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape : (seq_length, Batch_size) -> seq_length is the length of the input sentence, and we process the entire sequence at once\n",
    "        embedding = self.dropout(self.embedding(x))\n",
    "        # embedding shape : (seq_length, Batch_size, embedding_size)\n",
    "        outputs, (hidden, cell) = self.rnn(embedding)\n",
    "        # outputs shape : (seq_length, Batch_size, hidden_size)\n",
    "        # hidden shape : (num_layers, Batch_size, hidden_size)\n",
    "        # cell shape : (num_layers, Batch_size, hidden_size)\n",
    "        return hidden, cell\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, output_size, num_layers, p):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = nn.Dropout(p)\n",
    "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=p)\n",
    "        self.fc_out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, hidden, cell):\n",
    "        # x shape : (1, Batch_size)  -> we process one time step at a time\n",
    "        x = x.unsqueeze(0) #that why we added one dimention \n",
    "        # x shape : (1, Batch_size, 1)\n",
    "        embedding = self.dropout(self.embedding(x))\n",
    "        # embedding shape : (1, Batch_size, embedding_size)\n",
    "        outputs, (hidden, cell) = self.rnn(embedding, (hidden, cell))\n",
    "        # outputs shape : (1, Batch_size, hidden_size)\n",
    "        predictions = self.fc_out(outputs.squeeze(0)) #remove the time step dimension for the linear layer\n",
    "        # predictions shape : (Batch_size, output_size)\n",
    "        return predictions, hidden, cell\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def__init__(self, encoder, decoder):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "    \n",
    "    def forward(self, source, target, teacher_force_ratio=0.5):\n",
    "        batch_size = source.shape[1]\n",
    "        target_len = target.shape[0]\n",
    "        target_vocab_size = self.decoder.output_size\n",
    "\n",
    "        outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(device)\n",
    "\n",
    "        hidden, cell = self.encoder(source)\n",
    "\n",
    "        # First input to the decoder is the <SOS> tokens\n",
    "        x = target[0,:]  # shape: (Batch_size)\n",
    "\n",
    "        for t in range(1, target_len):\n",
    "            output, hidden, cell = self.decoder(x, hidden, cell)\n",
    "            outputs[t] = output\n",
    "\n",
    "            # Decide if we are going to use teacher forcing or not\n",
    "            teacher_force = random.random() < teacher_force_ratio\n",
    "\n",
    "            # Get the highest predicted token from our predictions\n",
    "            best_guess = output.argmax(1) \n",
    "\n",
    "            # If teacher forcing, use actual next token as next input; if not, use predicted token\n",
    "            x = target[t] if teacher_force else best_guess\n",
    "\n",
    "        return outputs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DLC",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
