{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70fc9e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "\n",
    "# pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import spacy\n",
    "from collections import Counter\n",
    "\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from torchmetrics.functional.text import bleu_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93c9cbd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device mps\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "print(\"Device\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6751a509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Set Seeds for Reproducibility\n",
    "SEED = 1234\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# Check if MPS is available and set the seed\n",
    "if torch.backends.mps.is_available():\n",
    "    torch.mps.manual_seed(SEED)\n",
    "\n",
    "# Check if CUDA is available (just in case you move this code to a server later)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "523fe585",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f85013c194a34c96a98b0c20c0fb6911",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "315deb5d256c4c57b40fad5aa88bed30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['translation'],\n",
      "        num_rows: 40836715\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['translation'],\n",
      "        num_rows: 3000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['translation'],\n",
      "        num_rows: 3003\n",
      "    })\n",
      "})\n",
      "First training example: {'translation': {'en': 'Resumption of the session', 'fr': 'Reprise de la session'}}\n"
     ]
    }
   ],
   "source": [
    "# Download and load the WMT14 French-English dataset\n",
    "# This might take a few minutes as the dataset is large\n",
    "dataset = load_dataset(\"wmt14\", \"fr-en\")\n",
    "\n",
    "# Print the dataset structure to verify\n",
    "print(dataset)\n",
    "\n",
    "# Example: Inspect the first training example\n",
    "print(\"First training example:\", dataset['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "599d2df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_fr = spacy.load(\"fr_core_news_sm\")\n",
    "spacy_eng = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6811ce66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#First, we create a class to manage the mapping between words and IDs. This handles the \"80k vocabulary\" limit mentioned in the paper.\n",
    "\n",
    "class Vocabulary:\n",
    "    def __init__(self, freq_threshold=2, max_size=80000):\n",
    "        self.itos = {0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\", 3: \"<UNK>\"}\n",
    "        self.stoi = {\"<PAD>\": 0, \"<SOS>\": 1, \"<EOS>\": 2, \"<UNK>\": 3}\n",
    "        self.freq_threshold = freq_threshold\n",
    "        self.max_size = max_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.itos)\n",
    "\n",
    "    @staticmethod\n",
    "    def tokenizer_eng(text):\n",
    "        return [tok.text.lower() for tok in spacy_eng.tokenizer(text)]\n",
    "\n",
    "    @staticmethod\n",
    "    def tokenizer_fr(text):\n",
    "        return [tok.text.lower() for tok in spacy_fr.tokenizer(text)]\n",
    "\n",
    "    def build_vocabulary(self, sentence_list, tokenizer):\n",
    "        frequencies = Counter()\n",
    "        idx = 4 # Start index after special tokens\n",
    "\n",
    "        # 1. Count frequencies of all words\n",
    "        for sentence in sentence_list:\n",
    "            for word in tokenizer(sentence):\n",
    "                frequencies[word] += 1\n",
    "\n",
    "        # 2. Sort by frequency and keep top 'max_size' words\n",
    "        # This matches the paper's strategy of capping vocab size\n",
    "        common_words = frequencies.most_common(self.max_size - 4)\n",
    "\n",
    "        # 3. Add valid words to our dictionary\n",
    "        for word, count in common_words:\n",
    "            if count >= self.freq_threshold:\n",
    "                self.stoi[word] = idx\n",
    "                self.itos[idx] = word\n",
    "                idx += 1\n",
    "\n",
    "    def numericalize(self, text, tokenizer):\n",
    "        tokenized_text = tokenizer(text)\n",
    "        \n",
    "        return [\n",
    "            self.stoi[token] if token in self.stoi else self.stoi[\"<UNK>\"]\n",
    "            for token in tokenized_text\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7031c14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we define a PyTorch Dataset that takes the raw Hugging Face data and converts it into numbers using the Vocabulary class above.\n",
    "\n",
    "class WMT14Dataset_regular_order(Dataset):\n",
    "    def __init__(self, hf_dataset, source_vocab, target_vocab):\n",
    "        self.hf_dataset = hf_dataset\n",
    "        self.source_vocab = source_vocab\n",
    "        self.target_vocab = target_vocab\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.hf_dataset)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Get the pair: {'fr': '...', 'en': '...'}\n",
    "        pair = self.hf_dataset[index]['translation']\n",
    "        src_text = pair['en']\n",
    "        trg_text = pair['fr']\n",
    "\n",
    "        # Convert text to indices\n",
    "        # Add <SOS> at start and <EOS> at end\n",
    "        numericalized_source = [self.source_vocab.stoi[\"<SOS>\"]]\n",
    "        numericalized_source += self.source_vocab.numericalize(src_text, self.source_vocab.tokenizer_eng)\n",
    "        numericalized_source.append(self.source_vocab.stoi[\"<EOS>\"])\n",
    "\n",
    "        numericalized_target = [self.target_vocab.stoi[\"<SOS>\"]]\n",
    "        numericalized_target += self.target_vocab.numericalize(trg_text, self.target_vocab.tokenizer_fr)\n",
    "        numericalized_target.append(self.target_vocab.stoi[\"<EOS>\"])\n",
    "\n",
    "        return torch.tensor(numericalized_source), torch.tensor(numericalized_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "94baabeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WMT14Dataset(Dataset):\n",
    "    def __init__(self, hf_dataset, source_vocab, target_vocab):\n",
    "        self.hf_dataset = hf_dataset\n",
    "        self.source_vocab = source_vocab\n",
    "        self.target_vocab = target_vocab\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.hf_dataset)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        pair = self.hf_dataset[index]['translation']\n",
    "        src_text = pair['en']\n",
    "        trg_text = pair['fr']\n",
    "\n",
    "        # 1. Numericalize Source\n",
    "        # Get list of numbers: [4, 10, 55] (\"the\", \"cat\", \"sat\")\n",
    "        src_indices = self.source_vocab.numericalize(src_text, self.source_vocab.tokenizer_eng)\n",
    "        \n",
    "        # --- IMPLEMENTATION OF PAPER POINT #1: REVERSE INPUT ---\n",
    "        # Reverse the list: [55, 10, 4] (\"sat\", \"cat\", \"the\")\n",
    "        src_indices = src_indices[::-1] \n",
    "        \n",
    "        # Add special tokens\n",
    "        numericalized_source = [self.source_vocab.stoi[\"<SOS>\"]]\n",
    "        numericalized_source += src_indices\n",
    "        numericalized_source.append(self.source_vocab.stoi[\"<EOS>\"])\n",
    "\n",
    "        # 2. Numericalize Target (Do NOT reverse this)\n",
    "        trg_indices = self.target_vocab.numericalize(trg_text, self.target_vocab.tokenizer_fr)\n",
    "        \n",
    "        numericalized_target = [self.target_vocab.stoi[\"<SOS>\"]]\n",
    "        numericalized_target += trg_indices\n",
    "        numericalized_target.append(self.target_vocab.stoi[\"<EOS>\"])\n",
    "\n",
    "        return torch.tensor(numericalized_source), torch.tensor(numericalized_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d48cc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Since sentences have different lengths, we cannot simply stack them into a matrix.\n",
    "# We need a specific function (called collate_fn) to pad short sentences with zeros (the <PAD> token) so that every batch is rectangular.\n",
    "\n",
    "class MyCollate:\n",
    "    def __init__(self, pad_idx):\n",
    "        self.pad_idx = pad_idx\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        source = [item[0] for item in batch]\n",
    "        target = [item[1] for item in batch]\n",
    "\n",
    "        # Pad sequences to the max length in this batch\n",
    "        source = pad_sequence(source, batch_first=False, padding_value=self.pad_idx)\n",
    "        target = pad_sequence(target, batch_first=False, padding_value=self.pad_idx)\n",
    "\n",
    "        return source, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "74f7f75c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Subset Size: 10000\n",
      "Valid Subset Size: 1000\n",
      "Building English Vocabulary...\n",
      "Building French Vocabulary...\n",
      "Testing the pipeline...\n",
      "Source Shape: torch.Size([80, 32])\n",
      "Target Shape: torch.Size([86, 32])\n"
     ]
    }
   ],
   "source": [
    "# 1. Load Data\n",
    "# Select subsets: 10k for training, 1k for validation\n",
    "train_subset = dataset['train'].select(range(10000))\n",
    "valid_subset = dataset['validation'].select(range(1000))\n",
    "\n",
    "print(f\"Train Subset Size: {len(train_subset)}\")\n",
    "print(f\"Valid Subset Size: {len(valid_subset)}\")\n",
    "\n",
    "# 2. Build Vocabulary\n",
    "print(\"Building English Vocabulary...\")\n",
    "english_sentences = [item['translation']['en'] for item in train_subset]\n",
    "vocab_en = Vocabulary(freq_threshold=1, max_size=80000) # Lowered freq_threshold for smaller dataset 10k\n",
    "vocab_en.build_vocabulary(english_sentences, vocab_en.tokenizer_eng)\n",
    "\n",
    "\n",
    "print(\"Building French Vocabulary...\")\n",
    "french_sentences = [item['translation']['fr'] for item in train_subset]\n",
    "vocab_fr = Vocabulary(freq_threshold=1, max_size=80000) # Lowered freq_threshold for smaller dataset 10k\n",
    "vocab_fr.build_vocabulary(french_sentences, vocab_fr.tokenizer_fr)\n",
    "\n",
    "# 3. Create Dataset\n",
    "train_subsetset = WMT14Dataset(train_subset, vocab_en, vocab_fr)\n",
    "valid_subsetset = WMT14Dataset(valid_subset, vocab_en, vocab_fr)\n",
    "\n",
    "# 4. Create DataLoaders\n",
    "BATCH_SIZE = 32 # Reduced batch size for smaller dataset 10k\n",
    "pad_idx = vocab_en.stoi[\"<PAD>\"]\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_subsetset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=0,\n",
    "    shuffle=True,\n",
    "    collate_fn=MyCollate(pad_idx=pad_idx)\n",
    ")\n",
    "\n",
    "# 5. Test it\n",
    "print(\"Testing the pipeline...\")\n",
    "for src_batch, trg_batch in train_loader:\n",
    "    print(f\"Source Shape: {src_batch.shape}\") # Expect [Seq_Len, Batch_Size]\n",
    "    print(f\"Target Shape: {trg_batch.shape}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "def664d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, num_layers, p):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = nn.Dropout(p)\n",
    "        self.embedding = nn.Embedding(input_size, embedding_size) # dropiut after embedding corrupts the input word vectors to prevent reliance on specific features.\n",
    "        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=p) # LSTM layer, dropout between layers prevents deeper\n",
    "                                                                               #layers from co-adapting too strongly with shallower layers.\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape : (seq_length, Batch_size) -> seq_length is the length of the input sentence, and we process the entire sequence at once\n",
    "        embedding = self.dropout(self.embedding(x))\n",
    "        # embedding shape : (seq_length, Batch_size, embedding_size)\n",
    "        outputs, (hidden, cell) = self.rnn(embedding)\n",
    "        # outputs shape : (seq_length, Batch_size, hidden_size)\n",
    "        # hidden shape : (num_layers, Batch_size, hidden_size)\n",
    "        # cell shape : (num_layers, Batch_size, hidden_size)\n",
    "        return hidden, cell\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, output_size, num_layers, p):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = nn.Dropout(p)\n",
    "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=p)\n",
    "        self.fc_out = nn.Linear(hidden_size, output_size) #output_size = input_size of the decoder = size of target vocabulary\n",
    "        self.output_size = output_size\n",
    "        \n",
    "    def forward(self, x, hidden, cell):\n",
    "        # x shape : (1, Batch_size)  -> we process one time step at a time\n",
    "        x = x.unsqueeze(0) #that's why we added one dimention \n",
    "        # x shape : (1, Batch_size, 1)\n",
    "        embedding = self.dropout(self.embedding(x))\n",
    "        # embedding shape : (1, Batch_size, embedding_size)\n",
    "        outputs, (hidden, cell) = self.rnn(embedding, (hidden, cell))\n",
    "        # outputs shape : (1, Batch_size, hidden_size)\n",
    "        predictions = self.fc_out(outputs.squeeze(0)) #remove the time step dimension for the linear layer\n",
    "        # predictions shape : (Batch_size, output_size)\n",
    "        return predictions, hidden, cell\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "    \n",
    "    def forward(self, source, target, teacher_force_ratio=0.5):\n",
    "        batch_size = source.shape[1]\n",
    "        target_len = target.shape[0]\n",
    "        target_vocab_size = self.decoder.output_size\n",
    "\n",
    "        outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(device)\n",
    "\n",
    "        hidden, cell = self.encoder(source)\n",
    "\n",
    "        # First input to the decoder is the <SOS> tokens\n",
    "        x = target[0,:]  # shape: (Batch_size)\n",
    "\n",
    "        for t in range(1, target_len):\n",
    "            output, hidden, cell = self.decoder(x, hidden, cell)\n",
    "            outputs[t] = output\n",
    "\n",
    "            # Decide if we are going to use teacher forcing or not\n",
    "            teacher_force = random.random() < teacher_force_ratio\n",
    "\n",
    "            # Get the highest predicted token from our predictions\n",
    "            best_guess = output.argmax(1) \n",
    "\n",
    "            # If teacher forcing, use actual next token as next input; if not, use predicted token\n",
    "            x = target[t] if teacher_force else best_guess\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "50612aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to initialize weights\n",
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
    "\n",
    "# Helper function to change LR\n",
    "def adjust_learning_rate(optimizer, decay_factor=0.5):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] *= decay_factor\n",
    "    print(f\"üìâ Learning Rate decayed to: {optimizer.param_groups[0]['lr']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a117841e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Architecture Specs from Paper ---\n",
    "INPUT_DIM = len(vocab_en)\n",
    "OUTPUT_DIM = len(vocab_fr)\n",
    "ENC_EMB_DIM = 1000  # Paper used 1000\n",
    "DEC_EMB_DIM = 1000  # Paper used 1000\n",
    "HID_DIM = 1000      # Paper used 1000\n",
    "N_LAYERS = 4        # Paper used 4\n",
    "DROPOUT = 0.2       # Paper implies some regularization, usually 0.2 is safe\n",
    "\n",
    "# --- Setup ---\n",
    "# Use the correct index for <PAD> from your English vocabulary\n",
    "TRG_PAD_IDX = vocab_en.stoi[\"<PAD>\"]\n",
    "\n",
    "# Create Model\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, DROPOUT)\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, OUTPUT_DIM, N_LAYERS, DROPOUT)\n",
    "model = Seq2Seq(enc, dec).to(device)\n",
    "\n",
    "# Initialize Weights\n",
    "model.apply(init_weights)\n",
    "\n",
    "# --- Optimization Specs ---\n",
    "BATCH_SIZE = 32    # Paper used 128\n",
    "LEARNING_RATE = 0.7 # Paper used fixed 0.7 initially\n",
    "CLIP = 5            # Paper threshold for gradient norm\n",
    "TOTAL_EPOCHS = 8 # Paper trained for 7.5 epochs\n",
    "# Optimizer: SGD without momentum\n",
    "optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=TRG_PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2d62bcbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Loss: 6.705\n",
      "Epoch: 02 | Loss: 6.414\n",
      "Epoch: 03 | Loss: 6.313\n",
      "Epoch: 04 | Loss: 6.272\n",
      "Epoch: 05 | Loss: 6.247\n",
      "üìâ Learning Rate decayed to: 0.35\n",
      "üìâ Learning Rate decayed to: 0.175\n",
      "Epoch: 06 | Loss: 6.223\n",
      "üìâ Learning Rate decayed to: 0.0875\n",
      "üìâ Learning Rate decayed to: 0.04375\n",
      "Epoch: 07 | Loss: 6.191\n",
      "üìâ Learning Rate decayed to: 0.021875\n",
      "üìâ Learning Rate decayed to: 0.0109375\n",
      "Epoch: 08 | Loss: 6.184\n"
     ]
    }
   ],
   "source": [
    "# --- Training Loop ---\n",
    "for epoch in range(TOTAL_EPOCHS):\n",
    "    \n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    # We need to know when we are \"halfway\" through\n",
    "    num_batches = len(train_loader)\n",
    "    halfway_point = num_batches // 2\n",
    "    \n",
    "    for i, (src, trg) in enumerate(train_loader):\n",
    "        \n",
    "        # --- THE HALF-EPOCH CHECK ---\n",
    "        # If we are past epoch 5, we check if we are at the halfway point OR the end\n",
    "        if epoch >= 5:\n",
    "            # Check if we are exactly at the halfway mark of the batch list\n",
    "            if i == halfway_point:\n",
    "                adjust_learning_rate(optimizer, 0.5)\n",
    "        \n",
    "        # Standard Training Step\n",
    "        src, trg = src.to(device), trg.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, trg)\n",
    "        \n",
    "        output_dim = output.shape[-1]\n",
    "        output = output[1:].view(-1, output_dim)\n",
    "        trg = trg[1:].view(-1)\n",
    "        \n",
    "        loss = criterion(output, trg)\n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), CLIP)\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    # --- END OF EPOCH CHECK ---\n",
    "    # After the loop finishes (end of epoch), if we are past epoch 5, decay again\n",
    "    if epoch >= 5:\n",
    "        adjust_learning_rate(optimizer, 0.5)\n",
    "\n",
    "    print(f'Epoch: {epoch+1:02} | Loss: {epoch_loss / len(train_loader):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dece8434",
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_search_decode_single(model, sentence, vocab_src, vocab_trg, beam_size=2, max_len=50, device='cpu'):\n",
    "    model.eval()\n",
    "    \n",
    "    # 1. Prepare Source\n",
    "    if isinstance(sentence, str):\n",
    "        tokens = vocab_src.tokenizer_eng(sentence)\n",
    "    else:\n",
    "        tokens = [token.lower() for token in sentence]\n",
    "        \n",
    "    indices = [vocab_src.stoi.get(t, vocab_src.stoi[\"<UNK>\"]) for t in tokens]\n",
    "    indices = indices[::-1] # Reverse Input\n",
    "    indices = [vocab_src.stoi[\"<SOS>\"]] + indices + [vocab_src.stoi[\"<EOS>\"]]\n",
    "    \n",
    "    src_tensor = torch.LongTensor(indices).unsqueeze(1).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        encoder_hidden, encoder_cell = model.encoder(src_tensor)\n",
    "\n",
    "        # Hypothesis: (Score, [Sequence], Hidden, Cell)\n",
    "        hypotheses = [(0.0, [vocab_trg.stoi[\"<SOS>\"]], encoder_hidden, encoder_cell)]\n",
    "\n",
    "        for _ in range(max_len):\n",
    "            all_candidates = []\n",
    "            \n",
    "            for score, seq, hidden, cell in hypotheses:\n",
    "                if seq[-1] == vocab_trg.stoi[\"<EOS>\"]:\n",
    "                    all_candidates.append((score, seq, hidden, cell))\n",
    "                    continue\n",
    "                \n",
    "                input_tensor = torch.LongTensor([seq[-1]]).to(device)\n",
    "                \n",
    "                # Predict\n",
    "                prediction, new_h, new_c = model.decoder(input_tensor, hidden, cell)\n",
    "                \n",
    "                # prediction is [1, vocab_size]. We squeeze to make it [vocab_size]\n",
    "                prediction = prediction.squeeze(0) \n",
    "                \n",
    "                # Log Softmax over dimension 0 (the vocabulary)\n",
    "                log_probs = F.log_softmax(prediction, dim=0)\n",
    "                \n",
    "                # Get Top K\n",
    "                top_k_probs, top_k_ids = log_probs.topk(beam_size * 2)\n",
    "                \n",
    "\n",
    "                # Expand\n",
    "                for i in range(len(top_k_ids)):\n",
    "                    word_idx = top_k_ids[i].item()\n",
    "                    prob = top_k_probs[i].item()\n",
    "                    all_candidates.append((score + prob, seq + [word_idx], new_h, new_c))\n",
    "\n",
    "            # Prune\n",
    "            hypotheses = sorted(all_candidates, key=lambda x: x[0], reverse=True)[:beam_size]\n",
    "            \n",
    "            if all(h[1][-1] == vocab_trg.stoi[\"<EOS>\"] for h in hypotheses):\n",
    "                break\n",
    "\n",
    "    best_seq = hypotheses[0][1]\n",
    "    decoded_words = [vocab_trg.itos[idx] for idx in best_seq]\n",
    "    \n",
    "    if \"<SOS>\" in decoded_words: decoded_words.remove(\"<SOS>\")\n",
    "    if \"<EOS>\" in decoded_words: decoded_words = decoded_words[:decoded_words.index(\"<EOS>\")]\n",
    "        \n",
    "    return \" \".join(decoded_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b0197511",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_bleu(data_subset, model, vocab_src, vocab_trg, device, beam_size=2):\n",
    "    targets = []      # Ground Truths\n",
    "    predictions = []  # Model Outputs\n",
    "    \n",
    "    print(f\"Starting BLEU Evaluation on {len(data_subset)} samples (Beam={beam_size})...\")\n",
    "    \n",
    "    for i, datum in enumerate(data_subset):\n",
    "        if (i + 1) % 50 == 0:\n",
    "            print(f\"Processed {i + 1} sentences...\")\n",
    "\n",
    "        pair = datum['translation']\n",
    "        src_text = pair['en']\n",
    "        trg_text = pair['fr']\n",
    "        \n",
    "        # 1. Get Prediction\n",
    "        pred_sentence = beam_search_decode_single(\n",
    "            model,          \n",
    "            src_text,       \n",
    "            vocab_src, \n",
    "            vocab_trg, \n",
    "            beam_size, \n",
    "            max_len=50,\n",
    "            device=device\n",
    "        )\n",
    "        \n",
    "        # 2. Collect Data\n",
    "        # Targets must be a list of lists: [['ref_sentence']]\n",
    "        targets.append([trg_text])\n",
    "        predictions.append(pred_sentence)\n",
    "\n",
    "    # 3. Compute Score\n",
    "    print(\"Computing Score...\")\n",
    "    score = bleu_score(predictions, targets, n_gram=4)\n",
    "    return score.item() * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a8619a07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting BLEU Evaluation on 1000 samples (Beam=12)...\n",
      "Processed 50 sentences...\n",
      "Processed 100 sentences...\n",
      "Processed 150 sentences...\n",
      "Processed 200 sentences...\n",
      "Processed 250 sentences...\n",
      "Processed 300 sentences...\n",
      "Processed 350 sentences...\n",
      "Processed 400 sentences...\n",
      "Processed 450 sentences...\n",
      "Processed 500 sentences...\n",
      "Processed 550 sentences...\n",
      "Processed 600 sentences...\n",
      "Processed 650 sentences...\n",
      "Processed 700 sentences...\n",
      "Processed 750 sentences...\n",
      "Processed 800 sentences...\n",
      "Processed 850 sentences...\n",
      "Processed 900 sentences...\n",
      "Processed 950 sentences...\n",
      "Processed 1000 sentences...\n",
      "Computing Score...\n",
      "Final Single-Model BLEU: 0.00\n"
     ]
    }
   ],
   "source": [
    "# Run this after training\n",
    "final_score = evaluate_bleu(\n",
    "    valid_subset,\n",
    "    model,             \n",
    "    vocab_en, \n",
    "    vocab_fr, \n",
    "    device,            \n",
    "    beam_size=12\n",
    ")\n",
    "\n",
    "print(f\"Final Single-Model BLEU: {final_score:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "54f314ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Visualizing 3 Random Samples (SINGLE MODEL) ---\n",
      "\n",
      "Example 1:\n",
      "SRC (English): The Federal Security Service now spreads a big network of fake sites and there are tons of potential buyers of military weapons.\n",
      "TRG (French):  Le Service f√©d√©ral de s√©curit√© a diffus√© un immense r√©seau de faux sites et ramasse √† la pelle les personnes d√©sireuses d'acheter des armes de combat.\n",
      "PRED (Model):  , , , de . .\n",
      "--------------------------------------------------\n",
      "\n",
      "Example 2:\n",
      "SRC (English): One thing is certain: these new provisions will have a negative impact on voter turn-out.\n",
      "TRG (French):  Une chose est certaine: ces nouvelles dispositions influenceront n√©gativement le taux de participation.\n",
      "PRED (Model):  , , , de . . . . .\n",
      "--------------------------------------------------\n",
      "\n",
      "Example 3:\n",
      "SRC (English): Chantal Rouleau was one of the first women in Montreal to raise the alarm.\n",
      "TRG (French):  Chantal Rouleau a √©t√© l'une des premi√®res √©lues de Montr√©al √† tirer la sonnette d'alarme.\n",
      "PRED (Model):  , , de . . . . . .\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def show_random_translations(dataset, models, vocab_src, vocab_trg, device, n_samples=5, beam_size=2):\n",
    "    # Determine if we are using Single or Ensemble\n",
    "    is_ensemble = isinstance(models, list)\n",
    "    model_type = \"ENSEMBLE\" if is_ensemble else \"SINGLE MODEL\"\n",
    "    \n",
    "    print(f\"--- Visualizing {n_samples} Random Samples ({model_type}) ---\")\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        # 1. Pick a random example\n",
    "        idx = random.randint(0, len(dataset)-1)\n",
    "        pair = dataset.hf_dataset[idx]['translation'] # Access raw HF data\n",
    "        \n",
    "        src = pair['en']\n",
    "        trg = pair['fr']\n",
    "\n",
    "        pred = beam_search_decode_single(models, src, vocab_src, vocab_trg, beam_size, device=device)\n",
    "            \n",
    "        # 3. Print\n",
    "        print(f\"\\nExample {i+1}:\")\n",
    "        print(f\"SRC (English): {src}\")\n",
    "        print(f\"TRG (French):  {trg}\")\n",
    "        print(f\"PRED (Model):  {pred}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "# --- Usage Examples ---\n",
    "\n",
    "# 1. For Single Model\n",
    "show_random_translations(valid_subsetset, model, vocab_en, vocab_fr, device, n_samples=3)\n",
    "\n",
    "# 2. For Ensemble (Pass the list of 5 models)\n",
    "# show_random_translations(valid_dataset, ensemble_models, vocab_en, vocab_fr, DEVICE, n_samples=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "056aabcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- DIAGNOSIS: VISUAL INSPECTION ---\n",
      "Input:  Recently he took up the street organ and became St. Petersburg's music man, because he was ready for this complex role with all his Bohemian existence, philosophy and image.\n",
      "Target: Il a r√©cemment pris dans ses mains un orgue de Barbarie et il est devenu le symbole de cet instrument √† Saint-P√©tersbourg puisqu'il avait assez m√ªri pour ce r√¥le difficile de par son existence, sa philosophie et son image de boh√©mien.\n",
      "Output: , , , de\n",
      "------------------------------\n",
      "Input:  Store on a sofa\n",
      "Target: Une boutique depuis son canap√©\n",
      "Output: \n",
      "------------------------------\n",
      "Input:  There is a connection with the fact that an infant spends about 9 months in amniotic fluid in the womb; it is easier to get used to water after that.\n",
      "Target: Dans la mesure o√π le b√©b√© √©volue environ 9 mois dans le liquide lymphatique du ventre de sa m√®re, il lui est plus facile de s'habituer ensuite √† l'eau.\n",
      "Output: , , ,\n",
      "------------------------------\n",
      "Input:  According to legends, this is where Noah built his ark and Perseus saved the beauty Andromeda, with whom he lived a long and happy life.\n",
      "Target: Selon la l√©gende, c'est ici que No√© a construit son arche et que Pers√©e a sauv√© la belle Androm√®de, avec qui il v√©cut ici-m√™me une longue et heureuse vie.\n",
      "Output: , , ,\n",
      "------------------------------\n",
      "Input:  Jerusalem has the most holy places for the Jews as well - the Wailing Wall, which remained from a temple destroyed by the Romans in 70 AD.\n",
      "Target: J√©rusalem abrite √©galement le lieu le plus sacr√© des Juifs: le Mur des Lamentations qui constitue le reste d'un temple d√©truit par les Romains en 70 av. J-C.\n",
      "Output: , , ,\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Use the beam_search_decode_single function you defined\n",
    "model.eval()\n",
    "print(f\"--- DIAGNOSIS: VISUAL INSPECTION ---\")\n",
    "\n",
    "for i in range(5):\n",
    "    idx = random.randint(0, len(valid_subset)-1)\n",
    "    pair = valid_subset[idx]['translation']\n",
    "    src = pair['en']\n",
    "    trg = pair['fr']\n",
    "    \n",
    "    pred = beam_search_decode_single(model, src, vocab_en, vocab_fr, beam_size=12, device=device)\n",
    "    \n",
    "    print(f\"Input:  {src}\")\n",
    "    print(f\"Target: {trg}\")\n",
    "    print(f\"Output: {pred}\")\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "a6470da4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- DIAGNOSIS: TRAINING SET CHECK ---\n",
      "Starting BLEU Evaluation on 100 samples (Beam=2)...\n",
      "Processed 50 sentences...\n",
      "Processed 100 sentences...\n",
      "Computing Score...\n",
      "Training Set BLEU: 0.00\n"
     ]
    }
   ],
   "source": [
    "# Create a tiny subset of the TRAINING data (which the model has seen)\n",
    "train_debug_subset = dataset['train'].select(range(100))\n",
    "\n",
    "print(\"--- DIAGNOSIS: TRAINING SET CHECK ---\")\n",
    "train_score = evaluate_bleu(\n",
    "    train_debug_subset, \n",
    "    model, \n",
    "    vocab_en, \n",
    "    vocab_fr, \n",
    "    device, \n",
    "    beam_size=2\n",
    ")\n",
    "print(f\"Training Set BLEU: {train_score:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "3ea30053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====================\n",
      "DEBUG TRAINING (Seed: 1)\n",
      "====================\n",
      "Epoch 1 | Loss: 6.452 | PPL: 634.18\n",
      "Epoch 2 | Loss: 5.982 | PPL: 396.34\n",
      "Epoch 3 | Loss: 5.787 | PPL: 326.17\n",
      "Epoch 4 | Loss: 5.632 | PPL: 279.35\n",
      "Epoch 5 | Loss: 5.552 | PPL: 257.84\n",
      "Epoch 6 | Loss: 5.462 | PPL: 235.59\n",
      "Epoch 7 | Loss: 5.410 | PPL: 223.55\n",
      "Epoch 8 | Loss: 5.341 | PPL: 208.75\n",
      "Epoch 9 | Loss: 5.323 | PPL: 205.01\n",
      "Epoch 10 | Loss: 5.280 | PPL: 196.37\n"
     ]
    }
   ],
   "source": [
    "# --- REVISED HYPERPARAMETERS ---\n",
    "INPUT_DIM = len(vocab_en)\n",
    "OUTPUT_DIM = len(vocab_fr)\n",
    "ENC_EMB_DIM = 256  # Reduced from 1000 to save memory/speed up debugging\n",
    "DEC_EMB_DIM = 256\n",
    "HID_DIM = 512      # Reduced from 1000 to save memory\n",
    "N_LAYERS = 2       # Reduced from 4 to 2 (Easier to train on small data)\n",
    "DROPOUT = 0.5      # Increased Dropout slightly\n",
    "N_EPOCHS = 10      # Give it a bit more time\n",
    "CLIP = 1           # Tighter gradient clipping\n",
    "DEVICE = device  # Use the detected device\n",
    "# --- MEMORY FIX SETTINGS ---\n",
    "BATCH_SIZE = 32\n",
    "GRAD_ACCUMULATION_STEPS = 1 # Set to 1 for Adam to simplify debugging\n",
    "\n",
    "# Re-initialize DataLoader\n",
    "pad_idx = vocab_en.stoi[\"<PAD>\"]\n",
    "train_loader = DataLoader(\n",
    "    WMT14Dataset(train_subset, vocab_en, vocab_fr),\n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True, \n",
    "    num_workers=0, \n",
    "    collate_fn=MyCollate(pad_idx)\n",
    ")\n",
    "\n",
    "SEEDS = [1] # Let's train just ONE model first to verify it works\n",
    "\n",
    "def train_one_epoch(model, iterator, optimizer, criterion, clip):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for i, (src, trg) in enumerate(iterator):\n",
    "        src, trg = src.to(DEVICE), trg.to(DEVICE)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, trg)\n",
    "        \n",
    "        output_dim = output.shape[-1]\n",
    "        output = output[1:].view(-1, output_dim)\n",
    "        trg = trg[1:].view(-1)\n",
    "        \n",
    "        loss = criterion(output, trg)\n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator)\n",
    "\n",
    "# --- TRAINING LOOP ---\n",
    "for seed in SEEDS:\n",
    "    print(f\"\\n{'='*20}\")\n",
    "    print(f\"DEBUG TRAINING (Seed: {seed})\")\n",
    "    print(f\"{'='*20}\")\n",
    "    \n",
    "    # 1. Set Seed\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.backends.mps.is_available(): torch.mps.manual_seed(seed)\n",
    "    \n",
    "    # 2. Initialize Model (Smaller & Simpler for Debugging)\n",
    "    enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, DROPOUT)\n",
    "    dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM,OUTPUT_DIM, N_LAYERS, DROPOUT)\n",
    "    model = Seq2Seq(enc, dec).to(DEVICE)\n",
    "    model.apply(init_weights)\n",
    "    \n",
    "    # 3. OPTIMIZER FIX: Use Adam instead of SGD\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    # Ensure we use the French padding index\n",
    "    TRG_PAD_IDX = vocab_fr.stoi[\"<PAD>\"]\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=TRG_PAD_IDX)\n",
    "    \n",
    "    # 4. Train\n",
    "    for epoch in range(N_EPOCHS):\n",
    "        loss = train_one_epoch(model, train_loader, optimizer, criterion, CLIP)\n",
    "        print(f\"Epoch {epoch+1} | Loss: {loss:.3f} | PPL: {math.exp(loss):.2f}\")\n",
    "        \n",
    "    # 5. Save\n",
    "    torch.save(model.state_dict(), f\"debug_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f3a48acb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- DIAGNOSIS: VISUAL INSPECTION ---\n",
      "Input:  According to critics, Walmart can afford to sell the products cheaply partly because it pays little to its employees.\n",
      "Target: Selon les critiques, Walmart peut se permettre de vendre ses produits bon march√© notamment en raison des salaires bas de ses employ√©s.\n",
      "Output: n' ne qui vous les des sein mais mis ce un lignes rapidement int√©gration aux\n",
      "------------------------------\n",
      "Input:  But 300 people will come for culture, not 10,000. In the end, there's less management, money, everything dies out.\n",
      "Target: Mais pour le c√¥t√© culturel, 300 personnes viendront, 10¬†milles ne viendront pas et au bout du compte c'est moins de management, d'argent, tout d√©p√©rit.\n",
      "Output: car , a ne qui - a bon m√™me √† cas -t d' r√®gles , et garantir soutien nouvelles autorit√© communaut√© .\n",
      "------------------------------\n",
      "Input:  The buyer pays at an ATM.\n",
      "Target: L'acheteur effectue le paiement sur les bornes automatiques.\n",
      "Output: n' ne qui - a bon m√™me √† cas -t d' r√®gles , et garantir soutien nouvelles autorit√© communaut√© .\n",
      "------------------------------\n",
      "Input:  But human rights activists, asked the question repeatedly in the press before the law came into force: what will it actually achieve?\n",
      "Target: Mais, avant que la loi entre en vigueur, les d√©fenseurs des droits de l'homme ont d√©j√† √† de nombreuses reprises pos√© la question dans la presse: qu'est-ce que √ßa va donner?\n",
      "Output: proc√©dure √©conomique d√©cisions la outre par femmes les est proposition son intergouvernementale .\n",
      "------------------------------\n",
      "Input:  In Europe, there are many curious people, who go to art exhibits, concerts.\n",
      "Target: En Europe, il y a beaucoup de curieux qui vont voir des expositions, des concerts.\n",
      "Output: proc√©dure √©conomique d√©cisions la outre par femmes les est proposition son intergouvernementale .\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Use the beam_search_decode_single function you defined\n",
    "model.eval()\n",
    "print(f\"--- DIAGNOSIS: VISUAL INSPECTION ---\")\n",
    "\n",
    "for i in range(5):\n",
    "    idx = random.randint(0, len(valid_subset)-1)\n",
    "    pair = valid_subset[idx]['translation']\n",
    "    src = pair['en']\n",
    "    trg = pair['fr']\n",
    "    \n",
    "    pred = beam_search_decode_single(model, src, vocab_en, vocab_fr, beam_size=12, max_len=50, device=device)\n",
    "    \n",
    "    print(f\"Input:  {src}\")\n",
    "    print(f\"Target: {trg}\")\n",
    "    print(f\"Output: {pred}\")\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c9faf97b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- STARTING MICRO-TRAINING ---\n",
      "Epoch 10 | Loss: 4.9017\n",
      "Epoch 20 | Loss: 4.4378\n",
      "Epoch 30 | Loss: 4.0045\n",
      "Epoch 40 | Loss: 3.5894\n",
      "Epoch 50 | Loss: 3.0060\n",
      "Epoch 60 | Loss: 2.3533\n",
      "Epoch 70 | Loss: 1.4639\n",
      "Epoch 80 | Loss: 1.0707\n",
      "Epoch 90 | Loss: 0.6613\n",
      "Epoch 100 | Loss: 0.5569\n",
      "\n",
      "--- VISUAL CHECK (Should be Perfect) ---\n",
      "Input: Resumption of the session\n",
      "Target: Reprise de la session\n",
      "Pred:   madame la session\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Create Micro-Dataset (Only 50 items) ---\n",
    "micro_subset = dataset['train'].select(range(50)) # First 50 sentences only\n",
    "\n",
    "# Re-build vocab on JUST these 50 sentences to ensure no <UNK>s\n",
    "vocab_en_micro = Vocabulary(freq_threshold=1, max_size=1000)\n",
    "vocab_en_micro.build_vocabulary([item['translation']['en'] for item in micro_subset], vocab_en_micro.tokenizer_eng)\n",
    "\n",
    "vocab_fr_micro = Vocabulary(freq_threshold=1, max_size=1000)\n",
    "vocab_fr_micro.build_vocabulary([item['translation']['fr'] for item in micro_subset], vocab_fr_micro.tokenizer_fr)\n",
    "\n",
    "# Loaders\n",
    "pad_idx_micro = vocab_en_micro.stoi[\"<PAD>\"]\n",
    "micro_loader = DataLoader(\n",
    "    WMT14Dataset(micro_subset, vocab_en_micro, vocab_fr_micro),\n",
    "    batch_size=10, # Small batch size\n",
    "    shuffle=True, \n",
    "    collate_fn=MyCollate(pad_idx_micro)\n",
    ")\n",
    "\n",
    "# --- 2. Setup Model (Tiny Config) ---\n",
    "INPUT_DIM = len(vocab_en_micro)\n",
    "OUTPUT_DIM = len(vocab_fr_micro)\n",
    "ENC_EMB_DIM = 64  \n",
    "DEC_EMB_DIM = 64\n",
    "HID_DIM = 128     \n",
    "N_LAYERS = 1      # 1 Layer is enough for 50 sentences\n",
    "DROPOUT = 0.0     # NO DROPOUT (We WANT to overfit)\n",
    "\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, DROPOUT)\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, OUTPUT_DIM ,N_LAYERS, DROPOUT)\n",
    "model = Seq2Seq(enc, dec).to(device)\n",
    "model.apply(init_weights)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.005) # High LR to learn fast\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=vocab_fr_micro.stoi[\"<PAD>\"])\n",
    "\n",
    "# --- 3. Train for 100 Epochs (Force Memorization) ---\n",
    "print(\"--- STARTING MICRO-TRAINING ---\")\n",
    "for epoch in range(100):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for src, trg in micro_loader:\n",
    "        src, trg = src.to(device), trg.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, trg)\n",
    "        output_dim = output.shape[-1]\n",
    "        output = output[1:].view(-1, output_dim)\n",
    "        trg = trg[1:].view(-1)\n",
    "        loss = criterion(output, trg)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1} | Loss: {epoch_loss / len(micro_loader):.4f}\")\n",
    "\n",
    "# --- 4. Verify ---\n",
    "print(\"\\n--- VISUAL CHECK (Should be Perfect) ---\")\n",
    "# Pick the FIRST sentence (index 0)\n",
    "pair = micro_subset[0]['translation']\n",
    "src = pair['en']\n",
    "trg = pair['fr']\n",
    "pred = beam_search_decode_single(model, src, vocab_en_micro, vocab_fr_micro, beam_size=2, max_len=50, device=device)\n",
    "\n",
    "print(f\"Input: {src}\")\n",
    "print(f\"Target: {trg}\")\n",
    "print(f\"Pred:   {pred}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b3ea4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- CHECKING OTHER EXAMPLES ---\n",
      "[0] Input:  Resumption of the session\n",
      "    Target: Reprise de la session\n",
      "    Pred:   je vous avez parfaitement raison et je vais v√©rifier si tout cela n ' a effectivement pas √©t√© fait .\n",
      "--------------------\n",
      "[1] Input:  I declare resumed the session of the European Parliament adjourned on Friday 17 December 1999, and I would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period.\n",
      "    Target: Je d√©clare reprise la session du Parlement europ√©en qui avait √©t√© interrompue le vendredi 17 d√©cembre dernier et je vous renouvelle tous mes vux en esp√©rant que vous avez pass√© de bonnes vacances.\n",
      "    Pred:   je vous demande donc √† nouveau de faire le n√©cessaire pour que nous puissions disposer d' une cha√Æne n√©erlandaise .\n",
      "--------------------\n",
      "[2] Input:  Although, as you will have seen, the dreaded 'millennium bug' failed to materialise, still the people in a number of countries suffered a series of natural disasters that truly were dreadful.\n",
      "    Target: Comme vous avez pu le constater, le grand \"bogue de l'an 2000\" ne s'est pas produit. En revanche, les citoyens d'un certain nombre de nos pays ont √©t√© victimes de catastrophes naturelles qui ont vraiment √©t√© terribles.\n",
      "    Pred:   cependant , madame la pr√©sidente , ce que j' avais demand√© n' a pas √©t√© r√©alis√© .\n",
      "--------------------\n",
      "[3] Input:  You have requested a debate on this subject in the course of the next few days, during this part-session.\n",
      "    Target: Vous avez souhait√© un d√©bat √† ce sujet dans les prochains jours, au cours de cette p√©riode de session.\n",
      "    Pred:   cependant , je vous demande d' accord , je ferai comme m. evans l' a sugg√©r√© .\n",
      "--------------------\n",
      "[4] Input:  In the meantime, I should like to observe a minute' s silence, as a number of Members have requested, on behalf of all the victims concerned, particularly those of the terrible storms, in the various countries of the European Union.\n",
      "    Target: En attendant, je souhaiterais, comme un certain nombre de coll√®gues me l'ont demand√©, que nous observions une minute de silence pour toutes les victimes, des temp√™tes notamment, dans les diff√©rents pays de l'Union europ√©enne qui ont √©t√© touch√©s.\n",
      "    Pred:   cependant , madame la pr√©sidente , ce que j' avais demand√© n' a pas √©t√© r√©alis√© .\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"--- CHECKING OTHER EXAMPLES ---\")\n",
    "\n",
    "# Let's check the first 5 items, not just item 0\n",
    "for i in range(5):\n",
    "    pair = micro_subset[i]['translation']\n",
    "    src = pair['en']\n",
    "    trg = pair['fr']\n",
    "    \n",
    "    # Translate\n",
    "    pred = beam_search_decode_single(\n",
    "        model, \n",
    "        src, \n",
    "        vocab_en_micro, \n",
    "        vocab_fr_micro, \n",
    "        beam_size=1, # Try Greedy search (Beam 1) to see raw output\n",
    "        device=device    )\n",
    "    \n",
    "    print(f\"[{i}] Input:  {src}\")\n",
    "    print(f\"    Target: {trg}\")\n",
    "    print(f\"    Pred:   {pred}\")\n",
    "    print(\"-\" * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8629af1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vocab_en_micro' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# --- HYPERPARAMETERS ---\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m INPUT_DIM = \u001b[38;5;28mlen\u001b[39m(\u001b[43mvocab_en_micro\u001b[49m)\n\u001b[32m      3\u001b[39m OUTPUT_DIM = \u001b[38;5;28mlen\u001b[39m(vocab_fr_micro)\n\u001b[32m      4\u001b[39m ENC_EMB_DIM = \u001b[32m64\u001b[39m  \n",
      "\u001b[31mNameError\u001b[39m: name 'vocab_en_micro' is not defined"
     ]
    }
   ],
   "source": [
    "# --- HYPERPARAMETERS ---\n",
    "INPUT_DIM = len(vocab_en_micro)\n",
    "OUTPUT_DIM = len(vocab_fr_micro)\n",
    "ENC_EMB_DIM = 64  \n",
    "DEC_EMB_DIM = 64\n",
    "HID_DIM = 128     \n",
    "N_LAYERS = 1      \n",
    "DROPOUT = 0.0     \n",
    "\n",
    "# --- KEY CHANGE: BATCH SIZE 1 ---\n",
    "# This removes all padding confusion. \n",
    "# The Encoder reads ONLY the words, no zeros.\n",
    "micro_loader_single = DataLoader(\n",
    "    WMT14Dataset(micro_subset, vocab_en_micro, vocab_fr_micro),\n",
    "    batch_size=1,  # <--- MAGIC NUMBER\n",
    "    shuffle=True, \n",
    "    collate_fn=MyCollate(pad_idx_micro)\n",
    ")\n",
    "\n",
    "# Initialize\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, DROPOUT)\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM,INPUT_DIM, N_LAYERS, DROPOUT)\n",
    "model = Seq2Seq(enc, dec).to(DEVICE)\n",
    "model.apply(init_weights)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.005)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=vocab_fr_micro.stoi[\"<PAD>\"])\n",
    "\n",
    "print(\"--- TRAINING WITH BATCH SIZE 1 (NO PADDING) ---\")\n",
    "for epoch in range(50): # 50 Epochs should be enough for Batch Size 1\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for src, trg in micro_loader_single:\n",
    "        src, trg = src.to(DEVICE), trg.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, trg)\n",
    "        output_dim = output.shape[-1]\n",
    "        output = output[1:].view(-1, output_dim)\n",
    "        trg = trg[1:].view(-1)\n",
    "        loss = criterion(output, trg)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1} | Loss: {epoch_loss / len(micro_loader_single):.4f}\")\n",
    "\n",
    "# --- CHECK ---\n",
    "print(\"\\n--- VISUAL CHECK (BATCH 1) ---\")\n",
    "pair = micro_subset[0]['translation']\n",
    "src = pair['en']\n",
    "trg = pair['fr']\n",
    "pred = beam_search_decode_single(model, src, vocab_en_micro, vocab_fr_micro, beam_size=2, device=DEVICE)\n",
    "\n",
    "print(f\"Input: {src}\")\n",
    "print(f\"Target: {trg}\")\n",
    "print(f\"Pred:   {pred}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DLC",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
