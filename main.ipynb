{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "70fc9e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "\n",
    "# pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import spacy\n",
    "from collections import Counter\n",
    "\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from torchmetrics.functional.text import bleu_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "93c9cbd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device mps\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "print(\"Device\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6751a509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Set Seeds for Reproducibility\n",
    "SEED = 1234\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# Check if MPS is available and set the seed\n",
    "if torch.backends.mps.is_available():\n",
    "    torch.mps.manual_seed(SEED)\n",
    "\n",
    "# Check if CUDA is available (just in case you move this code to a server later)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "523fe585",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72c2fc9fa31543a78192475ab465026a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "604f64bbb148492db163c0f3155366be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['translation'],\n",
      "        num_rows: 40836715\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['translation'],\n",
      "        num_rows: 3000\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['translation'],\n",
      "        num_rows: 3003\n",
      "    })\n",
      "})\n",
      "First training example: {'translation': {'en': 'Resumption of the session', 'fr': 'Reprise de la session'}}\n"
     ]
    }
   ],
   "source": [
    "# Download and load the WMT14 French-English dataset\n",
    "# This might take a few minutes as the dataset is large\n",
    "dataset = load_dataset(\"wmt14\", \"fr-en\")\n",
    "\n",
    "# Print the dataset structure to verify\n",
    "print(dataset)\n",
    "\n",
    "# Example: Inspect the first training example\n",
    "print(\"First training example:\", dataset['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "599d2df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_fr = spacy.load(\"fr_core_news_sm\")\n",
    "spacy_eng = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6811ce66",
   "metadata": {},
   "outputs": [],
   "source": [
    "#First, we create a class to manage the mapping between words and IDs. This handles the \"80k vocabulary\" limit mentioned in the paper.\n",
    "\n",
    "class Vocabulary:\n",
    "    def __init__(self, freq_threshold=2, max_size=80000):\n",
    "        self.itos = {0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\", 3: \"<UNK>\"}\n",
    "        self.stoi = {\"<PAD>\": 0, \"<SOS>\": 1, \"<EOS>\": 2, \"<UNK>\": 3}\n",
    "        self.freq_threshold = freq_threshold\n",
    "        self.max_size = max_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.itos)\n",
    "\n",
    "    @staticmethod\n",
    "    def tokenizer_eng(text):\n",
    "        return [tok.text.lower() for tok in spacy_eng.tokenizer(text)]\n",
    "\n",
    "    @staticmethod\n",
    "    def tokenizer_fr(text):\n",
    "        return [tok.text.lower() for tok in spacy_fr.tokenizer(text)]\n",
    "\n",
    "    def build_vocabulary(self, sentence_list, tokenizer):\n",
    "        frequencies = Counter()\n",
    "        idx = 4 # Start index after special tokens\n",
    "\n",
    "        # 1. Count frequencies of all words\n",
    "        for sentence in sentence_list:\n",
    "            for word in tokenizer(sentence):\n",
    "                frequencies[word] += 1\n",
    "\n",
    "        # 2. Sort by frequency and keep top 'max_size' words\n",
    "        # This matches the paper's strategy of capping vocab size\n",
    "        common_words = frequencies.most_common(self.max_size - 4)\n",
    "\n",
    "        # 3. Add valid words to our dictionary\n",
    "        for word, count in common_words:\n",
    "            if count >= self.freq_threshold:\n",
    "                self.stoi[word] = idx\n",
    "                self.itos[idx] = word\n",
    "                idx += 1\n",
    "\n",
    "    def numericalize(self, text, tokenizer):\n",
    "        tokenized_text = tokenizer(text)\n",
    "        \n",
    "        return [\n",
    "            self.stoi[token] if token in self.stoi else self.stoi[\"<UNK>\"]\n",
    "            for token in tokenized_text\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7031c14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we define a PyTorch Dataset that takes the raw Hugging Face data and converts it into numbers using the Vocabulary class above.\n",
    "\n",
    "class WMT14Dataset_regular_order(Dataset):\n",
    "    def __init__(self, hf_dataset, source_vocab, target_vocab):\n",
    "        self.hf_dataset = hf_dataset\n",
    "        self.source_vocab = source_vocab\n",
    "        self.target_vocab = target_vocab\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.hf_dataset)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Get the pair: {'fr': '...', 'en': '...'}\n",
    "        pair = self.hf_dataset[index]['translation']\n",
    "        src_text = pair['en']\n",
    "        trg_text = pair['fr']\n",
    "\n",
    "        # Convert text to indices\n",
    "        # Add <SOS> at start and <EOS> at end\n",
    "        numericalized_source = [self.source_vocab.stoi[\"<SOS>\"]]\n",
    "        numericalized_source += self.source_vocab.numericalize(src_text, self.source_vocab.tokenizer_eng)\n",
    "        numericalized_source.append(self.source_vocab.stoi[\"<EOS>\"])\n",
    "\n",
    "        numericalized_target = [self.target_vocab.stoi[\"<SOS>\"]]\n",
    "        numericalized_target += self.target_vocab.numericalize(trg_text, self.target_vocab.tokenizer_fr)\n",
    "        numericalized_target.append(self.target_vocab.stoi[\"<EOS>\"])\n",
    "\n",
    "        return torch.tensor(numericalized_source), torch.tensor(numericalized_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "94baabeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WMT14Dataset(Dataset):\n",
    "    def __init__(self, hf_dataset, source_vocab, target_vocab):\n",
    "        self.hf_dataset = hf_dataset\n",
    "        self.source_vocab = source_vocab\n",
    "        self.target_vocab = target_vocab\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.hf_dataset)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        pair = self.hf_dataset[index]['translation']\n",
    "        src_text = pair['en']\n",
    "        trg_text = pair['fr']\n",
    "\n",
    "        # 1. Numericalize Source\n",
    "        # Get list of numbers: [4, 10, 55] (\"the\", \"cat\", \"sat\")\n",
    "        src_indices = self.source_vocab.numericalize(src_text, self.source_vocab.tokenizer_eng)\n",
    "        \n",
    "        # --- IMPLEMENTATION OF PAPER POINT #1: REVERSE INPUT ---\n",
    "        # Reverse the list: [55, 10, 4] (\"sat\", \"cat\", \"the\")\n",
    "        src_indices = src_indices[::-1] \n",
    "        \n",
    "        # Add special tokens\n",
    "        numericalized_source = [self.source_vocab.stoi[\"<SOS>\"]]\n",
    "        numericalized_source += src_indices\n",
    "        numericalized_source.append(self.source_vocab.stoi[\"<EOS>\"])\n",
    "\n",
    "        # 2. Numericalize Target (Do NOT reverse this)\n",
    "        trg_indices = self.target_vocab.numericalize(trg_text, self.target_vocab.tokenizer_fr)\n",
    "        \n",
    "        numericalized_target = [self.target_vocab.stoi[\"<SOS>\"]]\n",
    "        numericalized_target += trg_indices\n",
    "        numericalized_target.append(self.target_vocab.stoi[\"<EOS>\"])\n",
    "\n",
    "        return torch.tensor(numericalized_source), torch.tensor(numericalized_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9d48cc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Since sentences have different lengths, we cannot simply stack them into a matrix.\n",
    "# We need a specific function (called collate_fn) to pad short sentences with zeros (the <PAD> token) so that every batch is rectangular.\n",
    "\n",
    "class MyCollate:\n",
    "    def __init__(self, pad_idx):\n",
    "        self.pad_idx = pad_idx\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        source = [item[0] for item in batch]\n",
    "        target = [item[1] for item in batch]\n",
    "\n",
    "        # Pad sequences to the max length in this batch\n",
    "        source = pad_sequence(source, batch_first=False, padding_value=self.pad_idx)\n",
    "        target = pad_sequence(target, batch_first=False, padding_value=self.pad_idx)\n",
    "\n",
    "        return source, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "74f7f75c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Subset Size: 10000\n",
      "Valid Subset Size: 1000\n",
      "Building English Vocabulary...\n",
      "Building French Vocabulary...\n",
      "Testing the pipeline...\n",
      "Source Shape: torch.Size([80, 32])\n",
      "Target Shape: torch.Size([86, 32])\n"
     ]
    }
   ],
   "source": [
    "# 1. Load Data\n",
    "# Select subsets: 10k for training, 1k for validation\n",
    "train_subset = dataset['train'].select(range(10000))\n",
    "valid_subset = dataset['validation'].select(range(1000))\n",
    "\n",
    "print(f\"Train Subset Size: {len(train_subset)}\")\n",
    "print(f\"Valid Subset Size: {len(valid_subset)}\")\n",
    "\n",
    "# 2. Build Vocabulary\n",
    "print(\"Building English Vocabulary...\")\n",
    "english_sentences = [item['translation']['en'] for item in train_subset]\n",
    "vocab_en = Vocabulary(freq_threshold=1, max_size=80000) # Lowered freq_threshold for smaller dataset 10k\n",
    "vocab_en.build_vocabulary(english_sentences, vocab_en.tokenizer_eng)\n",
    "\n",
    "\n",
    "print(\"Building French Vocabulary...\")\n",
    "french_sentences = [item['translation']['fr'] for item in train_subset]\n",
    "vocab_fr = Vocabulary(freq_threshold=1, max_size=80000) # Lowered freq_threshold for smaller dataset 10k\n",
    "vocab_fr.build_vocabulary(french_sentences, vocab_fr.tokenizer_fr)\n",
    "\n",
    "# 3. Create Dataset\n",
    "train_subsetset = WMT14Dataset(train_subset, vocab_en, vocab_fr)\n",
    "valid_subsetset = WMT14Dataset(valid_subset, vocab_en, vocab_fr)\n",
    "\n",
    "# 4. Create DataLoaders\n",
    "BATCH_SIZE = 32 # Reduced batch size for smaller dataset 10k\n",
    "pad_idx = vocab_en.stoi[\"<PAD>\"]\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_subsetset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=0,\n",
    "    shuffle=True,\n",
    "    collate_fn=MyCollate(pad_idx=pad_idx)\n",
    ")\n",
    "\n",
    "# 5. Test it\n",
    "print(\"Testing the pipeline...\")\n",
    "for src_batch, trg_batch in train_loader:\n",
    "    print(f\"Source Shape: {src_batch.shape}\") # Expect [Seq_Len, Batch_Size]\n",
    "    print(f\"Target Shape: {trg_batch.shape}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "def664d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, num_layers, p):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = nn.Dropout(p)\n",
    "        self.embedding = nn.Embedding(input_size, embedding_size) # dropiut after embedding corrupts the input word vectors to prevent reliance on specific features.\n",
    "        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=p) # LSTM layer, dropout between layers prevents deeper\n",
    "                                                                               #layers from co-adapting too strongly with shallower layers.\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape : (seq_length, Batch_size) -> seq_length is the length of the input sentence, and we process the entire sequence at once\n",
    "        embedding = self.dropout(self.embedding(x))\n",
    "        # embedding shape : (seq_length, Batch_size, embedding_size)\n",
    "        outputs, (hidden, cell) = self.rnn(embedding)\n",
    "        # outputs shape : (seq_length, Batch_size, hidden_size)\n",
    "        # hidden shape : (num_layers, Batch_size, hidden_size)\n",
    "        # cell shape : (num_layers, Batch_size, hidden_size)\n",
    "        return hidden, cell\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_size, embedding_size, hidden_size, output_size, num_layers, p):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = nn.Dropout(p)\n",
    "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
    "        self.rnn = nn.LSTM(embedding_size, hidden_size, num_layers, dropout=p)\n",
    "        self.fc_out = nn.Linear(hidden_size, output_size) #output_size = input_size of the decoder = size of target vocabulary\n",
    "        self.output_size = output_size\n",
    "        \n",
    "    def forward(self, x, hidden, cell):\n",
    "        # x shape : (1, Batch_size)  -> we process one time step at a time\n",
    "        x = x.unsqueeze(0) #that's why we added one dimention \n",
    "        # x shape : (1, Batch_size, 1)\n",
    "        embedding = self.dropout(self.embedding(x))\n",
    "        # embedding shape : (1, Batch_size, embedding_size)\n",
    "        outputs, (hidden, cell) = self.rnn(embedding, (hidden, cell))\n",
    "        # outputs shape : (1, Batch_size, hidden_size)\n",
    "        predictions = self.fc_out(outputs.squeeze(0)) #remove the time step dimension for the linear layer\n",
    "        # predictions shape : (Batch_size, output_size)\n",
    "        return predictions, hidden, cell\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "    \n",
    "    def forward(self, source, target, teacher_force_ratio=0.5):\n",
    "        batch_size = source.shape[1]\n",
    "        target_len = target.shape[0]\n",
    "        target_vocab_size = self.decoder.output_size\n",
    "\n",
    "        outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(device)\n",
    "\n",
    "        hidden, cell = self.encoder(source)\n",
    "\n",
    "        # First input to the decoder is the <SOS> tokens\n",
    "        x = target[0,:]  # shape: (Batch_size)\n",
    "\n",
    "        for t in range(1, target_len):\n",
    "            output, hidden, cell = self.decoder(x, hidden, cell)\n",
    "            outputs[t] = output\n",
    "\n",
    "            # Decide if we are going to use teacher forcing or not\n",
    "            teacher_force = random.random() < teacher_force_ratio\n",
    "\n",
    "            # Get the highest predicted token from our predictions\n",
    "            best_guess = output.argmax(1) \n",
    "\n",
    "            # If teacher forcing, use actual next token as next input; if not, use predicted token\n",
    "            x = target[t] if teacher_force else best_guess\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "50612aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to initialize weights\n",
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
    "\n",
    "# Helper function to change LR\n",
    "def adjust_learning_rate(optimizer, decay_factor=0.5):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] *= decay_factor\n",
    "    print(f\"üìâ Learning Rate decayed to: {optimizer.param_groups[0]['lr']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a117841e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Architecture Specs from Paper ---\n",
    "INPUT_DIM = len(vocab_en)\n",
    "OUTPUT_DIM = len(vocab_fr)\n",
    "ENC_EMB_DIM = 1000  # Paper used 1000\n",
    "DEC_EMB_DIM = 1000  # Paper used 1000\n",
    "HID_DIM = 1000      # Paper used 1000\n",
    "N_LAYERS = 4        # Paper used 4\n",
    "DROPOUT = 0.2       # Paper implies some regularization, usually 0.2 is safe\n",
    "\n",
    "# --- Setup ---\n",
    "# Use the correct index for <PAD> from your English vocabulary\n",
    "TRG_PAD_IDX = vocab_en.stoi[\"<PAD>\"]\n",
    "\n",
    "# Create Model\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, DROPOUT)\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, OUTPUT_DIM, N_LAYERS, DROPOUT)\n",
    "model = Seq2Seq(enc, dec).to(device)\n",
    "\n",
    "# Initialize Weights\n",
    "model.apply(init_weights)\n",
    "\n",
    "# --- Optimization Specs ---\n",
    "BATCH_SIZE = 32    # Paper used 128\n",
    "LEARNING_RATE = 0.7 # Paper used fixed 0.7 initially\n",
    "CLIP = 5            # Paper threshold for gradient norm\n",
    "TOTAL_EPOCHS = 8 # Paper trained for 7.5 epochs\n",
    "# Optimizer: SGD without momentum\n",
    "optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=TRG_PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2d62bcbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Loss: 6.705\n",
      "Epoch: 02 | Loss: 6.414\n",
      "Epoch: 03 | Loss: 6.313\n",
      "Epoch: 04 | Loss: 6.272\n",
      "Epoch: 05 | Loss: 6.247\n",
      "üìâ Learning Rate decayed to: 0.35\n",
      "üìâ Learning Rate decayed to: 0.175\n",
      "Epoch: 06 | Loss: 6.223\n",
      "üìâ Learning Rate decayed to: 0.0875\n",
      "üìâ Learning Rate decayed to: 0.04375\n",
      "Epoch: 07 | Loss: 6.191\n",
      "üìâ Learning Rate decayed to: 0.021875\n",
      "üìâ Learning Rate decayed to: 0.0109375\n",
      "Epoch: 08 | Loss: 6.184\n"
     ]
    }
   ],
   "source": [
    "# --- Training Loop ---\n",
    "for epoch in range(TOTAL_EPOCHS):\n",
    "    \n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    # We need to know when we are \"halfway\" through\n",
    "    num_batches = len(train_loader)\n",
    "    halfway_point = num_batches // 2\n",
    "    \n",
    "    for i, (src, trg) in enumerate(train_loader):\n",
    "        \n",
    "        # --- THE HALF-EPOCH CHECK ---\n",
    "        # If we are past epoch 5, we check if we are at the halfway point OR the end\n",
    "        if epoch >= 5:\n",
    "            # Check if we are exactly at the halfway mark of the batch list\n",
    "            if i == halfway_point:\n",
    "                adjust_learning_rate(optimizer, 0.5)\n",
    "        \n",
    "        # Standard Training Step\n",
    "        src, trg = src.to(device), trg.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, trg)\n",
    "        \n",
    "        output_dim = output.shape[-1]\n",
    "        output = output[1:].view(-1, output_dim)\n",
    "        trg = trg[1:].view(-1)\n",
    "        \n",
    "        loss = criterion(output, trg)\n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), CLIP)\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    # --- END OF EPOCH CHECK ---\n",
    "    # After the loop finishes (end of epoch), if we are past epoch 5, decay again\n",
    "    if epoch >= 5:\n",
    "        adjust_learning_rate(optimizer, 0.5)\n",
    "\n",
    "    print(f'Epoch: {epoch+1:02} | Loss: {epoch_loss / len(train_loader):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "dece8434",
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_search_decode_single(model, sentence, vocab_src, vocab_trg, beam_size=2, max_len=50, device='cpu'):\n",
    "    model.eval()\n",
    "    \n",
    "    # 1. Prepare Source\n",
    "    if isinstance(sentence, str):\n",
    "        tokens = vocab_src.tokenizer_eng(sentence)\n",
    "    else:\n",
    "        tokens = [token.lower() for token in sentence]\n",
    "        \n",
    "    indices = [vocab_src.stoi.get(t, vocab_src.stoi[\"<UNK>\"]) for t in tokens]\n",
    "    indices = indices[::-1] # Reverse Input\n",
    "    indices = [vocab_src.stoi[\"<SOS>\"]] + indices + [vocab_src.stoi[\"<EOS>\"]]\n",
    "    \n",
    "    src_tensor = torch.LongTensor(indices).unsqueeze(1).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        encoder_hidden, encoder_cell = model.encoder(src_tensor)\n",
    "\n",
    "        # Hypothesis: (Score, [Sequence], Hidden, Cell)\n",
    "        hypotheses = [(0.0, [vocab_trg.stoi[\"<SOS>\"]], encoder_hidden, encoder_cell)]\n",
    "\n",
    "        for _ in range(max_len):\n",
    "            all_candidates = []\n",
    "            \n",
    "            for score, seq, hidden, cell in hypotheses:\n",
    "                if seq[-1] == vocab_trg.stoi[\"<EOS>\"]:\n",
    "                    all_candidates.append((score, seq, hidden, cell))\n",
    "                    continue\n",
    "                \n",
    "                input_tensor = torch.LongTensor([seq[-1]]).to(device)\n",
    "                \n",
    "                # Predict\n",
    "                prediction, new_h, new_c = model.decoder(input_tensor, hidden, cell)\n",
    "                \n",
    "                # --- FIX STARTS HERE ---\n",
    "                # prediction is [1, vocab_size]. We squeeze to make it [vocab_size]\n",
    "                prediction = prediction.squeeze(0) \n",
    "                \n",
    "                # Log Softmax over dimension 0 (the vocabulary)\n",
    "                log_probs = F.log_softmax(prediction, dim=0)\n",
    "                \n",
    "                # Get Top K\n",
    "                top_k_probs, top_k_ids = log_probs.topk(beam_size * 2)\n",
    "                # --- FIX ENDS HERE ---\n",
    "\n",
    "                # Expand\n",
    "                for i in range(len(top_k_ids)):\n",
    "                    word_idx = top_k_ids[i].item() # Now this works!\n",
    "                    prob = top_k_probs[i].item()\n",
    "                    all_candidates.append((score + prob, seq + [word_idx], new_h, new_c))\n",
    "\n",
    "            # Prune\n",
    "            hypotheses = sorted(all_candidates, key=lambda x: x[0], reverse=True)[:beam_size]\n",
    "            \n",
    "            if all(h[1][-1] == vocab_trg.stoi[\"<EOS>\"] for h in hypotheses):\n",
    "                break\n",
    "\n",
    "    best_seq = hypotheses[0][1]\n",
    "    decoded_words = [vocab_trg.itos[idx] for idx in best_seq]\n",
    "    \n",
    "    if \"<SOS>\" in decoded_words: decoded_words.remove(\"<SOS>\")\n",
    "    if \"<EOS>\" in decoded_words: decoded_words = decoded_words[:decoded_words.index(\"<EOS>\")]\n",
    "        \n",
    "    return \" \".join(decoded_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "b0197511",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_bleu(data_subset, model, vocab_src, vocab_trg, device, beam_size=2):\n",
    "    targets = []      # Ground Truths\n",
    "    predictions = []  # Model Outputs\n",
    "    \n",
    "    print(f\"Starting BLEU Evaluation on {len(data_subset)} samples (Beam={beam_size})...\")\n",
    "    \n",
    "    for i, datum in enumerate(data_subset):\n",
    "        if (i + 1) % 50 == 0:\n",
    "            print(f\"Processed {i + 1} sentences...\")\n",
    "\n",
    "        pair = datum['translation']\n",
    "        src_text = pair['en']\n",
    "        trg_text = pair['fr']\n",
    "        \n",
    "        # 1. Translate (Using the correct argument order!)\n",
    "        pred_sentence = beam_search_decode_single(\n",
    "            model,          # Model First\n",
    "            src_text,       # Sentence Second\n",
    "            vocab_src, \n",
    "            vocab_trg, \n",
    "            beam_size, \n",
    "            max_len=50,\n",
    "            device=device\n",
    "        )\n",
    "        \n",
    "        # 2. Collect Data\n",
    "        # Targets must be a list of lists: [['ref_sentence']]\n",
    "        targets.append([trg_text])\n",
    "        predictions.append(pred_sentence)\n",
    "\n",
    "    # 3. Compute Score\n",
    "    print(\"Computing Score...\")\n",
    "    score = bleu_score(predictions, targets, n_gram=4)\n",
    "    return score.item() * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "a8619a07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting BLEU Evaluation on 1000 samples (Beam=2)...\n",
      "Processed 50 sentences...\n",
      "Processed 100 sentences...\n",
      "Processed 150 sentences...\n",
      "Processed 200 sentences...\n",
      "Processed 250 sentences...\n",
      "Processed 300 sentences...\n",
      "Processed 350 sentences...\n",
      "Processed 400 sentences...\n",
      "Processed 450 sentences...\n",
      "Processed 500 sentences...\n",
      "Processed 550 sentences...\n",
      "Processed 600 sentences...\n",
      "Processed 650 sentences...\n",
      "Processed 700 sentences...\n",
      "Processed 750 sentences...\n",
      "Processed 800 sentences...\n",
      "Processed 850 sentences...\n",
      "Processed 900 sentences...\n",
      "Processed 950 sentences...\n",
      "Processed 1000 sentences...\n",
      "Computing Score...\n",
      "Final Single-Model BLEU: 0.00\n"
     ]
    }
   ],
   "source": [
    "# Run this after training\n",
    "final_score = evaluate_bleu(\n",
    "    valid_subset,      # Your 1000 sample validation set\n",
    "    model,             # Your single trained model\n",
    "    vocab_en, \n",
    "    vocab_fr, \n",
    "    device,            # Your device variable\n",
    "    beam_size=2\n",
    ")\n",
    "\n",
    "print(f\"Final Single-Model BLEU: {final_score:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "54f314ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Visualizing 3 Random Samples (SINGLE MODEL) ---\n",
      "\n",
      "Example 1:\n",
      "SRC (English): Under the right rulers everything was different. Kings may not have understood culture very well, but they understood that they needed to stick with the right experts.\n",
      "TRG (French):  Sous le r√®gne des bons tsars, ce n'√©tait pas comme √ßa, les tsars ne comprenaient pas bien la culture, mais ils comprenaient qu'il fallait garder les bons experts.\n",
      "PRED (Model):  , , , de . . . .\n",
      "--------------------------------------------------\n",
      "\n",
      "Example 2:\n",
      "SRC (English): He's worked with great players.\n",
      "TRG (French):  Il a c√¥toy√© de grands joueurs.\n",
      "PRED (Model):  de\n",
      "--------------------------------------------------\n",
      "\n",
      "Example 3:\n",
      "SRC (English): Nonetheless, the Ministry of the Interior asserts that the situation of the spread of illegal arms is under control.\n",
      "TRG (French):  Toutefois, le Minist√®re de l'Int√©rieur affirme qu'il contr√¥le la diffusion des armes ill√©gales.\n",
      "PRED (Model):  , , , de . . . . .\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def show_random_translations(dataset, models, vocab_src, vocab_trg, device, n_samples=5, beam_size=2):\n",
    "    # Determine if we are using Single or Ensemble\n",
    "    is_ensemble = isinstance(models, list)\n",
    "    model_type = \"ENSEMBLE\" if is_ensemble else \"SINGLE MODEL\"\n",
    "    \n",
    "    print(f\"--- Visualizing {n_samples} Random Samples ({model_type}) ---\")\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        # 1. Pick a random example\n",
    "        idx = random.randint(0, len(dataset)-1)\n",
    "        pair = dataset.hf_dataset[idx]['translation'] # Access raw HF data\n",
    "        \n",
    "        src = pair['en']\n",
    "        trg = pair['fr']\n",
    "        \n",
    "        # 2. Translate\n",
    "        if is_ensemble:\n",
    "            pred = beam_search_decode_ensemble(models, src, vocab_src, vocab_trg, beam_size, device=device)\n",
    "        else:\n",
    "            pred = beam_search_decode_single(models, src, vocab_src, vocab_trg, beam_size, device=device)\n",
    "            \n",
    "        # 3. Print\n",
    "        print(f\"\\nExample {i+1}:\")\n",
    "        print(f\"SRC (English): {src}\")\n",
    "        print(f\"TRG (French):  {trg}\")\n",
    "        print(f\"PRED (Model):  {pred}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "# --- Usage Examples ---\n",
    "\n",
    "# 1. For Single Model\n",
    "show_random_translations(valid_subsetset, model, vocab_en, vocab_fr, device, n_samples=3)\n",
    "\n",
    "# 2. For Ensemble (Pass the list of 5 models)\n",
    "# show_random_translations(valid_dataset, ensemble_models, vocab_en, vocab_fr, DEVICE, n_samples=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DLC",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
